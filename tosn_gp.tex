\documentclass{acmtrans2m}
\usepackage{preamble}
\catcode`\@=11
\usepackage{booktabs}
\newcommand{\dnt}[1]{_{{#1}}}
\newcommand{\Kl}[1]{K_{\text{#1}}}
\usepackage{nicefrac}
\usepackage{algorithmic}
\usepackage{algorithm}
\newcommand{\cmmnt}{// }


% \usepackage{epsfig}
% \usepackage{array}
% \usepackage{amsmath,amssymb}
% \usepackage{verbatim}
% \usepackage{epic}
% \usepackage{multirow}
% \usepackage[dvips]{color}
% 
% \definecolor{blue}{rgb}{0,0,0.5}
% \definecolor{magenta}{rgb}{0.5,0,0.5}
% 
% \newcommand{\argmin}{\operatornamewithlimits{argmin}}
% \newcommand{\ud}{\mathrm{d}}
% 
% % The following designed for probabilities with long arguments
% 
% \newcommand{\Prob}[2]{P\!\left(#1\left|#2\right.\right)}
% \newcommand{\ProbF}[3]{P\!\left(#1\!=\!#2\left|#3\right.\right)}
% \newcommand{\p}[2]{p\!\left(\left.#1\right|#2\right)}
% \newcommand{\pF}[3]{p\!\left(#1\!=\!#2\left|#3\right.\right)} 
% \newcommand{\mean}[2]{\vect{m}\!\left(\left.#1\right|#2\right)}
% \newcommand{\cov}[3]{\mat{C}!\left(#1\!=\!#2\left|#3\right.\right)} 
% 
% % The 'F', as in 'pF', is for 'Full'
% \newcommand{\vect}[1]{\boldsymbol{#1}}
% 
% \newcommand{\vx}{\vect{x}}
% \newcommand{\vxst}{\vx_\star}
% \newcommand{\vxD}{\vect{x}_d}
% 
% \newcommand{\vt}{\vect{t}}
% \newcommand{\vtst}{\vect{t}_\star}
% \newcommand{\vtD}{\vect{t}_d}
% 
% \newcommand{\vy}{\vect{y}}
% \newcommand{\vyst}{\vy_\star}
% \newcommand{\vyD}{\vect{y}_d}
% 
% \newcommand{\vlst}{\vect{l}_\star}
% \newcommand{\vlD}{\vect{l}_d}
% 
% \newcommand{\vz}{\vect{z}}
% \newcommand{\vzst}{\vz_\star}
% \newcommand{\vzD}{\vz_d}
% 
% \newcommand{\vmu}{\vect{\mu}}
% 
% \newcommand{\vph}{\vect{\phi}}
% \newcommand{\vphS}{\vph_s}
% \newcommand{\phst}{\phi_\star}
% 
% \newcommand{\Dt}{\vect{\mathfrak{a}}}
% \newcommand{\Kt}[1]{\vect{\mathfrak{b}}_{\star,#1}}
% 
% \newcommand{\Tt}{\vect{\mathfrak{c}}}
% \newcommand{\ntS}{\vect{\mathfrak{n}}_s}
% \newcommand{\nt}[1]{\vect{\mathfrak{n}}_{S,#1}}
% \newcommand{\mt}[1]{\vect{\mathfrak{m}}_{S,#1}}
% \newcommand{\Nt}{\mat{\mathfrak{N}}_s}
% 
% \newcommand{\mat}[1]{\mathbf{#1}}
% \newcommand{\N}[3]{\mat{N}\!\left(#1;#2,#3\right)}
% \newcommand{\Ph}[3]{\Phi\!\left(#1;#2,#3\right)}
% \newcommand{\dd}[2]{\delta\!\left(#1-#2\right)}
% \newcommand{\ones}[1]{\mat{1}_{#1}}
% \newcommand{\eye}[1]{\mat{I}_{#1}}
% \newcommand{\tr}{\mathrm{T}}
% \newcommand{\trace}{\operatorname{tr}}
% \newcommand{\defequal}{\triangleq}
% \newcommand{\degree}{^\circ}
% 
% 
% \providecommand{\norm}[1]{\left\lvert#1\right\rvert}
% \providecommand{\normw}[1]{\left\lVert#1\right\rVert_w}
% \providecommand{\card}[1]{\left\lvert#1\right\rvert}
% 
% \DeclareMathOperator{\chol}{chol}
% \DeclareMathOperator{\diag}{diag}

%\newcommand{\tr}{\ensuremath{\mathsf{T}}}

\markboth{M. A. Osborne et al.}{Real-Time Information Processing of Environmental Sensor Network Data}

\title{Real-Time Information Processing of Environmental Sensor Network Data using Bayesian Gaussian Processes\footnote{A preliminary version of the work presented in this paper appeared as: Osborne, M. A., Rogers, A., Ramchurn, S., Roberts, S. J. and Jennings, N. R. (2008) Towards Real-Time Information Processing of Sensor Network Data using Computationally Efficient Multi-output Gaussian Processes. In: International Conference on Information Processing in Sensor Networks (IPSN 2008), April 2008, St. Louis, Missouri, USA. pp. 109-120.}}

\author{M. A. Osborne and S. J. Roberts\\
Department of Engineering Science\\
University of Oxford\\
Oxford, OX1 3PJ, UK.\\
{\tt{\{mosb,sjrob\}@robots.ox.ac.uk}}\\
\and\\
A. Rogers and N. R. Jennings\\
School of Electronics and Computer Science\\
University of Southampton\\
Southampton, SO17 1BJ, UK.\\
{\tt{\{acr,nrj\}@ecs.soton.ac.uk}}\\
}

\begin{abstract}

\noindent In this paper, we consider the problem faced by a sensor network operator who must infer, in real-time, the value of some environmental parameter that is being monitored at discrete points in space and time by a sensor network. We describe a powerful and generic approach built upon an efficient multi-output Gaussian process (GP) that facilitates this information acquisition and processing. Our algorithm allows effective inference even with minimal domain knowledge, and we further introduce a formulation of Bayesian Monte Carlo to permit the principled management of the hyperparameters introduced by our flexible models. We demonstrate how our methods can be applied even in the presence of various problematic data features, including cases where our data is delayed, intermittently missing, censored and/or correlated, and we show how a decision theoretic method can be used to determine the optimal selection of observations in order to maintain a desired level of accuracy in our inference. We validate our approach using data collected from three networks of weather sensors and show that it yields better inference performance than both conventional independent Gaussian processes and the Kalman filter. Finally, we analyse the computational speed of our algorithm. We show that our formalism that efficiently re-uses previous computations by following an online update procedure as new data sequentially arrives results in four fold increase in computational speed in the largest cases considered, and enables all of the information processing tasks described in this paper to be performed in real-time.
\end{abstract}

\category{C.2.4}{Computer Communication Networks}{Distributed
Systems~--~Distributed Applications}

\terms{Algorithms, Design, Performance}

\keywords{Learning of models from data, Gaussian processes, information processing, adaptive sampling}

\begin{document}

\maketitle

\section{Introduction}

\noindent Sensor networks have recently generated a great deal of research interest within the computer and physical sciences, and their use for the scientific monitoring of remote and hostile environments is increasingly common-place. While early sensor networks were a simple evolution of existing automated data loggers that collected data for later off-line scientific analysis, more recent sensor networks now make their data available in real-time through the internet, and increasingly perform some form of real-time data processing or aggregation to provide useful summary information to users \cite{esn}.

Providing real-time access to sensor data in this way presents many novel challenges; not least the need for self-describing data formats, and standard protocols such that the existence and capabilities of sensors can be advertised to potential users. However, more significantly for us, many of the information processing tasks that would previously have been performed off-line by the expert owner or single user of an environmental sensor network (such as detecting faulty sensors, performing `data cleaning' to remove erroneous readings, fusing noisy measurements from several sensors, and deciding how frequently data should be collected), must now be performed autonomously in real-time. Furthermore, since such information processing is likely to be performed within centralised sensor repositories --- of which Weather Underground (\url{http://www.wunderground.com}, pachube (http://www.pachube.com/) and Microsoft Research's SensorMap (\url{http://atom.research.microsoft.com/sensormap/}) are embryonic examples --- and will be applied to open sensor networks where additional sensors may be deployed at any time, and existing sensors may be removed, repositioned or updated after deployment (such as the rooftop weather sensors within the Weather Underground), these information processing tasks may have to be performed with only limited knowledge of the precise location, reliability, and accuracy of each sensor.

Now, many of the information processing tasks described above have previously been tackled by applying principled Bayesian methodologies from the academic literature of geospatial statistics and machine learning: specifically, kriging \cite{cressie} and Gaussian processes \cite{GPsBook}. However, due to the computational complexity of these approaches, to date they have largely been used off-line in order to analyse and re-design existing sensor networks (e.g. to reduce maintenance costs by removing the least informative sensors from an existing sensor network \cite{fuentes}, or to find the optimum placement of a small number of sensors, after a trial deployment of a larger number has collected data indicating their spatial correlation \cite{guestrin1}). Alternatively, they have been applied to single sensors and have ignored the correlations that exist between different sensors within the network \cite{1525857,usac}. Thus, there is a clear need for more computationally efficient algorithms, in order that this information processing can be performed at scale in real-time.

Against this background, this paper describes our work developing just such an algorithm. More specifically, we present a novel iterative formulation of a multi-output Gaussian process (GP) that uses a computationally efficient implementation of {\em Bayesian Monte Carlo} to marginalise hyperparameters, efficiently re-uses previous computations by following an online update procedure as new data sequentially arrives, and uses a principled `windowing' of data in order to maintain a reasonably sized data set. We use this GP to build a probabilistic model of the environmental variables being measured by sensors within the network, that is tolerant to data that may be missing, delayed, censored and/or correlated. This model allows us to then perform information processing tasks including: modelling the accuracy of the sensor readings, predicting the value of missing sensor readings, predicting how the monitored environmental variables will evolve in the near future, and performing active sampling by deciding when and from which sensor to acquire readings. We validate our approach using data collected from three networks of weather sensors and show that it yields better inference performance than both conventional independent single-output Gaussian processes, that model each sensor independently, and the Kalman filter. Finally, we analyse the computational speed of our algorithm. We show that our formalism that efficiently re-uses previous computations by following an online update procedure as new data sequentially arrives results in four fold increase in computational speed in the largest cases considered, and enables all of the information processing tasks described in this paper to be performed in real-time.

The remainder of this paper is organised as follows: Section \ref{sec_info} describes the information processing problem that we face. Section \ref{sec_gp} presents our Gaussian process formulation, and Section \ref{sec_implementation} describes the two sensor networks used to validate it. In Section \ref{sec_evaluation} we present experimental results using data from these networks, and in Section \ref{sec_computation} we present results on the computational cost of our algorithm. Finally, related work is discussed in Section \ref{sec_related}, and we conclude in Section \ref{sec_conclusion}.

\section{The Information Processing Problem}\label{sec_info}

\noindent As discussed above, we require that our algorithm be able to autonomously perform data acquisition and information processing despite having only limited specific knowledge of each of the sensors in its local neighbourhood (e.g. their precise location, reliability, and accuracy). To this end, we require that it explicitly represents:
\begin{enumerate}
\item The uncertainty in the estimated values of environmental variables being measured, noting that sensor readings will always incorporate some degree of measurement noise.
\item The correlations or delays that exist between sensor readings; sensors that are close to one another, or in similar environments, will tend to make similar readings, while many physical processes involving moving fields (such as the movement of weather fronts) will induce delays between sensors.
\end{enumerate}
We then require that it uses this representation in order to:
\begin{enumerate}
\item Perform regression and prediction of environmental variables; that is, interpolate between sensor readings to predict variables at missing sensors (i.e. sensors that have failed or are unavailable through network outages), and perform short term prediction in order to support decision making.
\item Perform efficient active sampling by selecting when to take a reading, and which sensor to read from, such that the minimum number of sensor readings are used to maintain the estimated uncertainty in environmental variables below a specified threshold (or similarly, to minimise uncertainty given a constrained number of sensor readings). Such constraints may reflect the computational limitations of the processor on which the algorithm is running, or alternatively, where the algorithm is actually controlling the sensors within the network, it may reflect the constrained power consumption of the sensors themselves.
\end{enumerate}
More specifically, the problem that we face can be cast as a multivariate regression and decision problem in which we have $l=1,\ldots, L$ environmental variables $x_l \in \mathbb{R}$ of interest (such as air temperature, wind speed or direction specified at different sensor locations). We assume a set of $N$ potentially noisy sensor readings, $ \bigl\{\bigl[[l_1,t_1],z_1\bigr],\ldots,\bigl[[l_N,t_N],z_N\bigr]\bigr\}$, in which we, for example, observe the value $z_1$ for the ${l_1}^\text{th}$ variable at time $t_1$, whose true unknown value is $y_1$. Where convenient, we may group the inputs as $x=[l,t]$. Note that we do not require that all the variables are observed at the same time, nor do we impose any discretisation of our observations into regularly spaced time steps. We define our vector of observations as $\vzD\defequal[z_1,\ldots,z_N]$ of variables labelled by $\vlD\defequal[l_1,\ldots,l_N]$ at times $\vtD\defequal[t_1,\ldots,t_N]$. Given this data, we are interested in inferring the vector of values $\vyst$ for any other vector of variables labelled by $\vlst$ at times $\vtst$.

\section{Gaussian Processes}\label{sec_gp}

\noindent Multivariate regression problems of the form described above are increasingly being  addressed using Gaussian processes (GPs). These represent a powerful way to perform Bayesian inference about functions; we consider our environmental variables as just such a function \cite{GPsBook}. This function takes as inputs the variable label and time pair $x$ and produces as output the variable's value $y$. In this work, we will assume that our inputs are always known (e.g. our data is time-stamped), and will incorporate them into our background knowledge, or context, $I$. A GP is then a generalised multivariate Gaussian prior distribution over the (potentially infinite number of) outputs of this function:
\begin{align*}%\label{eq:GPDefn}
\p{\vy}{\vmu,\,K,\,I} & \defequal \N{\vy}{\vmu}{\mat{K}}\nonumber\\
& \defequal\frac{1}{\sqrt{\det{2\pi\mat{K}}}}\,\exp \left(-\frac{1}{2}\,(\vy-\vmu)^\tr\,\mat{K}^{-1}\,(\vy-\vmu)\right)\,.
\end{align*}
It is specified by prior mean and covariance functions, which generate $\vmu$ and $\mat{K}$.
%Maybe drop the following line?
 The multivariate Gaussian distribution is qualified for this role due to the fact that both its marginal probabilities and conditional probabilities are themselves Gaussian. This allows us to produce analytic posterior distributions for outputs of interest, conditioned on whatever sensor readings have been observed. Furthermore, this posterior distribution will have both a predictive mean and a variance to explicitly represent our uncertainty.

While the fundamental theory of GPs is well established (see \citeN{GPsBook} for example), there is much scope for the development of computationally efficient implementations especially for applications such as sensor networks which involve large numbers of sensors collecting large quantities of timestamped data. To this end, in this work we present a novel on-line formalism of a multi-dimensional GP that allows us to model the correlations between sensor readings, and to update this model on-line as new observations are sequentially available. In the next sections we describe the covariance functions that we use to represent correlations and delays between sensor readings, the {\em Bayesian Monte Carlo} method that we use to marginalise the hyperparameters of these covariance functions, and how we efficiently update the model as new data is received, by reusing the results of previous computations, and applying a principled `windowing' of our data series.

\subsection{Mean and Covariance Functions}

\noindent The prior mean of a GP represents whatever we expect for our function before seeing any data. Throughout this paper, we will take this as a function constant in time for each sensor, such that $\mu([l,t])=\mu_l$. The \emph{covariance function} of a GP specifies the correlation between any pair of outputs. This can then be used to generate a covariance matrix over our set of observations and predictants. Fortunately, there exist a wide variety of functions that can serve in this purpose \cite{Abrahamsen,stein2005stc}, which can then be combined and modified in a further multitude of ways. This gives us a great deal of flexibility in our modelling of functions, with covariance functions available to model periodicity, delay, noise and long-term drifts. 

Examples of covariance functions are given by the Mat\'{e}rn class \cite{GPsBook}, given by
\begin{equation} \label{eq:Maternnu}
\Kl{Mtn}(t,t';h,w,\nu)\defequal h^2\,\frac{2^{1-\nu}}{\Gamma(\nu)}\,\biggl(\sqrt{2\nu}\,r\biggr)^\nu \mathfrak{K}_\nu\biggl(\sqrt{2\nu}\,r\biggr)
\end{equation}
where $r = \left|\frac{t-t'}{w}\right|$, $\nu>0$ is a smoothness hyperparameter (larger $\nu$ implies smoother functions) and $\mathfrak{K}_\nu$ is the modified Bessel function. Fortunately, \eqref{eq:Maternnu} simplifies for half-integer $\nu$, to give, for the example of $\nu=\nicefrac{5}{2}$
\begin{equation} \label{eq:matern}
\Kl{Mtn}(t,t';h,\,w,\,\nu=5/2) \defequal h^2 \left(1+\sqrt{5}r+\frac{5r^2}{3}\right)\exp\left(-\sqrt{5}r\right)\,,
\end{equation}
and its modification for periodic signals
\begin{equation} \label{eq:permatern}
\Kl{per-Mtn}(t,t';h,\,w,\,\nu=5/2) \defequal h^2 \left(1+\sqrt{5}r_{\text{per}}+\frac{5r_{\text{per}}^2}{3}\right)\exp\left(-\sqrt{5}r_{\text{per}}\right)\,,
\end{equation}
where $r_{\text{per}} = \sin \pi \left|\frac{t-t'}{w}\right|$. This modification of $r$ to accommodate periodicity is readily made for other covariance functions.

There are many ways to construct a covariance over a multi-dimensional input space, such as our two-dimensional input space $x=[l,t]$. We will typically exploit the fact that a product of covariances is a covariance in its own right, and write
 $$
K\big([l,t],[l',t']\big) =  \Kl{t}(t,t')\,\Kl{l}(l,l')\,,
$$
taking independent covariance terms over time and sensor label. If the number of sensors is not too large, we can arbitrarily represent the covariance matrix $\Kl{l}$ over sensor labels using the spherical decomposition \cite{PinheiroBates}. This allows us to arbitrarily represent any possible covariance over sensor labels. For a covariance matrix $W$, we write
\begin{equation} \label{eq:spherical_W}
 W={S}\,\diag({\tau})\,.
\end{equation}
Here ${S}$ is an upper triangular matrix, whose $n$th column contains the spherical coordinates in $\mathbb{R}^n$ of a point on the hypersphere $\mathbb{S}^{n-1}$, followed by the requisite number of zeros. As an example, $S$ for a four dimensional space is
\begin{equation} \label{eq:spherical}
 S=\left[ \begin{array}{llll}
1 & \cos{\theta_1} 	& \cos{\theta_2} 			& \cos{\theta_4} \\
0 & \sin{\theta_1} 	& \sin{\theta_2}\,\cos{\theta_3} 	& \sin{\theta_4}\,\cos{\theta_5} \\
0 & 0 			& \sin{\theta_2}\,\sin{\theta_3} 	& \sin{\theta_4}\,\sin{\theta_5}\,\cos{\theta_6} \\
0 & 0 			& 0				 	& \sin{\theta_4}\,\sin{\theta_5}\,\sin{\theta_6} 
   \end{array} \right]\,.
\end{equation}
This form ensures that $S^\tr S$ has ones across its diagonal and hence all other entries may be thought of as akin to correlation coefficients, lying between $-1$ and $1$. Meanwhile, ${\tau}$ is a vector of the scales for each dimension. If $W$ is the covariance matrix parameterising the Mahalanobis distance, $\tau$ represents a vector of input scales for each input, and the off-diagonal elements of $S^\tr S$ express the correlation amongst inputs. If instead we have used $W$ as a parameterisation of $\Kl{l}([1,\,\ldots,\,L],[1,\,\ldots,\,L])$, $\tau$ gives a vector of output scales corresponding to the different values $l=[1,\,\ldots,\,L]$. $S^\tr S$ gives the correlation coefficients for each pair of values $l$ might take.

These intuitive connections provide the motivation for the use of this parameterisation. Finally, note that the total number of hyperparameters required by the spherical parameterisation is $\frac{1}{2}\,n\,(n+1)$ if the side length of $W$ is $n$. Note that this is the same number as would be required to parameterise the non-zero elements of an upper triangular Cholesky factor $R=\chol{W}$ directly. The spherical parametrisation hence requires a large number of hyperparameters, but allows us to express any possible covariance matrix.

Finally, to represent measurement noise, we further extend the covariance function to
\begin{equation*}
V([l,t],[l',t'])\defequal K([l,t],[l',t'])+ \sigma^2\,\dd{[l,t]}{[l',t']}\,,
\end{equation*}
where $\dd{}{}$ is the Kronecker delta and $\sigma^2$ represents the variance of additive Gaussian noise.

The flexibility of our model comes at the cost of the introduction of a number of hyperparameters, which we collectively denote as $\phi$. These include correlation hyperparameters, along with others such as the periods and amplitudes of each covariance term (i.e. $w$ and $h$) and the noise deviation $\sigma$. The constant prior means $\mu_1,\ldots,\mu_L$ are also included as additional hyperparameters. Taking these hyperparameters as given and using the properties of the Gaussian distribution, we are able to write our predictive equations as
\begin{equation} \label{eq:GPMeanVar}
\p{\vxst}{\vyD,\,\phi,\,I}=\N{\vxst}{\vect{m}_\ast}{\mat{C}_\ast}\,,
\end{equation}
where, collecting our inputs as $\vxst\defequal[\vlst,\vtst]$ and $\vxD\defequal[\vlD,\vtD]$, we have:
\begin{align} 
%\vect{m}_\star=\vmu_\phi([\vlst,\vtst])+\\\mat{K}_\phi([\vlst,\vtst],[\vlD,\vtD])\,\mat{K}_\phi([\vlD,\vtD],[\vlD,\vtD])^{-1}\,(\vyD-\vect{\mu}_\phi([\vlD,\vtD]))
\vect{m}_\ast&=\vmu(\vxst;\phi)+\mat{K}(\vxst,\vxD;\phi)\mat{V}(\vxD,\vxD;\phi)^{-1}(\vyD-\vect{\mu}(\vxD;\phi))\label{eq:GPMean}\\
\mat{C}_\ast&=\mat{K}(\vxst,\vxst;\phi)-\mat{K}(\vxst,\vxD;\phi)\mat{V}(\vxD,\vxD;\phi)^{-1}\mat{K}(\vxD,\vxst;\phi)\label{eq:GPVar}\,.
\end{align}
For notational convenience, we will often drop the dependence upon $\phi$ from our covariance and mean functions.

\subsection{Marginalisation}

\noindent Of course, it is rare that we can be certain a priori about the values of our hyperparameters. One common approach to dealing with this uncertainty is to explore the possible space of hyperparameters (using a local search method such as gradient ascent) in order to select the set that exhibit the greatest log likelihood \cite{GPsBook}. However, this approach fails to represent the uncertainty in the value of these hyperparameters, and thus, it fails to represent the true uncertainty in the final predictions of the environmental variables beings measured.

To correctly represent this uncertainty we must marginalise over $\phi$, and thus, rather than equation \eqref{eq:GPMeanVar}, we must consider
\begin{equation} \label{eq:Marginalising}
 \p{\vyst}{\vzD,I}=\frac{\int\ud\phi\, \p{\vyst}{\vzD,\phi,I}\p{\vzD}{\phi,I}\p{\phi}{I}}
{\int\ud\phi\, \p{\vzD}{\phi,I}\p{\phi}{I}}\,.
\end{equation}
Unfortunately, both our likelihood $\p{\vzD}{\phi,I}$ and predictions $\p{\vyst}{\vzD,\phi,I}$ exhibit non-trivial dependence upon $\phi$ and so our integrals are non-analytic. As such, we resort to quadrature, which inevitably involves evaluating the two quantities
\begin{equation}
\begin{split}
q(\phi)& \defequal \p{\vyst}{\vzD,\,\phi,\,I} \\
r(\phi)& \defequal \p{\vzD}{\phi,\,I}
\end{split}
\end{equation}
at a set of sample points $\vph_s=\{\phi_1,\ldots,\phi_\eta\}$, giving $\vect{q}_s\defequal q(\vph_s)$ and $\vect{r}_s\defequal r(\vph_s)$. Of course, this evaluation is a computationally expensive operation. Defining the vector of all possible function inputs as $\vph$, we clearly can't afford to evaluate $\vect{q}\defequal q(\vph)$ or $\vect{r}\defequal r(\vph)$. Note that the more complex our model, and hence the greater the number of hyperparameters, the higher the dimension of the hyperparameter space we must sample in. As such, the complexity of models we can practically consider is limited by the curse of dimensionality. We can view our sparse sampling as introducing a form of uncertainty about the functions $q$ and $r$, which we can again address using Bayesian probability theory. 

To this end, we apply \emph{Bayesian Monte Carlo}, and thus, assign further GP priors to $q$ and $r$ \cite{BZMonteCarlo}. This choice is motivated by the fact that variables over which we have a multivariate Gaussian distribution are joint Gaussian with any projections of those variables. As such, given that integration is a projection, we can use our computed samples $\vect{q}_s$ in order to perform Gaussian process regression about the value of integrals over $q(\phi)$, and similarly for $r$. Note that the quantity we wish to perform inference about, 
\begin{equation}
 \psi\defequal\p{\vyst}{\vect{q},\,\vect{r},\,\vzD,I}= \frac{\int\ud\phst\, q(\phst)\,r(\phst)\,\p{\phst}{I}}
{\int\ud\phst\,r(\phst)\,\p{\phst}{I}}\,,
\end{equation} 
possesses richer structure than that previously considered using Bayesian Monte Carlo techniques. In our case, $r(\phi)$ appears in both our numerator and denominator integrals, introducing correlations between the values we estimate for them. The correlation structure of this system is illustrated in figure \ref{fig:BMC2}. 

% Define $\vph_{\overline{s}}\defequal\{\phi:\phi\notin \vph_s\}$ and $\vect{q}_{\overline{s}}\defequal q(\vph_{\overline{s}})$ and $\vect{r}_{\overline{s}}\defequal r(\vph_{\overline{s}})$. Of course, we know neither $\vect{q}_{\overline{s}}$ nor $\vect{r}_{\overline{s}}$, 
In considering any problem of inference, we need to be clear about both what information we have and which uncertain variables we are interested in. In our case, both function values, $\vect{q}_s$ and $\vect{r}_s$, and their locations, $\vph_s$, represent valuable pieces of knowledge\footnote{As discussed by \citeN{MCUnsound}, traditional, frequentist Monte Carlo effectively ignores the information content of $\vph_s$, leading to several unsatisfactory features.}. As with our convention above, we will take knowledge of sample locations $\vph_s$ to be implicit within $I$. We respectively define $\vect{m}^{(q)}$ and $\vect{m}^{(r)}$ as the means for $\vect{q}$ and $\vect{r}$ conditioned on $\vect{q}_s$ and $\vect{r}_s$ from \eqref{eq:GPMean}, $\mat{C}^{(r)}$ and $\mat{C}^{(r)}$ the similarly conditional covariances from \eqref{eq:GPVar}. The ultimate quantity of our interest is then
\begin{align}
&\p{\vyst}{\vect{q}_s,\,\vect{r}_s,\,\vzD,\,I} \nonumber\\  
& = \iiint \p{\vyst}{\vect{q},\,\vect{r},\,\vzD,I}\,\p{\psi}{\vect{q},\,\vect{r},\,I}\,\p{\vect{q}}{\vect{q}_s,\,I}\,\p{\vect{r}}{\vect{r}_s,\,I}\,\ud\psi\, \ud \vect{q}\,\ud \vect{r} \nonumber\\
& = \iiint \psi\,\dd{\psi}{\frac{\int \ud\phst\,q_\ast\,r_\ast\,\p{\phst}{I}}
{\int \ud\phst\,r_\ast\,\p{\phst}{I}}}\, \N{\vect{q}}{\vect{m}^{(q)}}{\mat{C}^{(q)}}\, \N{\vect{r}}{\vect{m}^{(r)}}{\mat{C}^{(r)}}\,\ud\psi\, \ud \vect{q}\,\ud \vect{r} \nonumber \\
& = \int \frac{\int \ud\phst\,m^{(q)}_\ast\,r_\ast\,\p{\phst}{I}}
{\int \ud\phst\,r_\ast\,\p{\phst}{I}}\, \N{\vect{r}}{\vect{m}^{(r)}}{\mat{C}^{(r)}}\,\ud \vect{r}\,.
\end{align}
Here, unfortunately, our integration over $r$ becomes nonanalytic. However, we can employ a maximum {\it a posteriori} approximation by assuming
$$
\N{\vect{r}}{\vect{m}^{(r)}}{\mat{C}^{(r)}} \simeq \dd{\vect{r}}{\vect{m}^{(r)}}\,.
$$
Essentially, we assume that our uncertainty in the likelihood function is small and trust that our more correct treatment of the uncertainty in the prediction function $q$ is sufficient. Given that we are likely to be uncertain about $r(\phi)$ at exactly the same $\phi$ we are uncertain about $q(\phi)$, those points far removed from our samples $\vphS$, this approximation is not unreasonable.  Before we can state our consequent results, to each of our hyperparameters we assign a Gaussian prior distribution (or if our hyperparameter is restricted to the positive reals, we instead assign a Gaussian distribution to its log) given by
\begin{align}
 \p{\phi}{I} & \defequal \N{\phi}{\vect{\nu}}{\mat{\lambda}^\tr \mat{\lambda}} \label{eq:phiprior}\,.
\end{align}
Note that the intuitive spherical parameterisation \eqref{eq:spherical} assists with the elicitation of priors over its hyperparameters. We then assign a {\em squared exponential} covariance function for the GP over both $q$ and $r$ given by
\begin{align}
 K(\phi,\phi') & \defequal \N{\phi}{\phi'}{\mat{w}^\tr \mat{w}} \label{eq:BMCsqdexp}\,.
\end{align}

 \begin{figure}
\centering%
	\begin{pspicture}(-6,0)(6,5)%
	%\showgrid
	\GM@Inode{0}{4}{1}%	
	%\rput(I){\rput(0,-2){\GM@node{X}}}   \GM@label[angle=90]{X}{$X$}
	\rput(0,2){\GM@detnode{psi}}   \GM@label[angle=-90]{psi}{$\Psi$}

% NB \Phi is the actual value of the hyperparameters -- doesn't make any sense to write \Phi_i

	\rput(psi){\rput(-5,-1.75){\GM@plate[plateLabelPos=tl]{3.5}{1.5}{$j \in S$}}}
	\rput(psi){\rput(-5,0.25){\GM@plate[plateLabelPos=bl]{3.5}{1.5}{$i \in S$}}}
	\rput(psi){\rput(-2.5,1.25){\GM@detnode[observed=true]{Qi}}}   \GM@label[angle=-90]{Qi}{$Q_i$}
	\rput(Qi){\rput(0,-2.5){\GM@detnode[observed=true]{Ri}}}   \GM@label[angle=90]{Ri}{$R_j$}
	\rput(Qi){\rput(-1.5,0){\GM@node[observed=true]{phiQi}}}   \GM@label[angle=180]{phiQi}{$\phi_{i}$}
	\rput(Ri){\rput(-1.5,0){\GM@node[observed=true]{phiRi}}}   \GM@label[angle=180]{phiRi}{$\phi_{j}$}

	\rput(psi){\rput(1.5,-1.75){\GM@plate[plateLabelPos=tr]{3.5}{1.5}{$j' \not\in S$}}}
	\rput(psi){\rput(1.5,0.25){\GM@plate[plateLabelPos=br]{3.5}{1.5}{$i' \not\in S$}}}
	\rput(psi){\rput(2.5,1.25){\GM@detnode{Qj}}}   \GM@label[angle=-90]{Qj}{$Q_{i'}$}
	\rput(Qj){\rput(0,-2.5){\GM@detnode{Rj}}}   \GM@label[angle=90]{Rj}{$R_{j'}$}
	\rput(Qj){\rput(1.5,0){\GM@node[observed=true]{phiQ'}}}   \GM@label[angle=0]{phiQ'}{$\phi'_{i}$}
	\rput(Rj){\rput(1.5,0){\GM@node[observed=true]{phiR'}}}   \GM@label[angle=0]{phiR'}{$\phi'_{j}$}


	\pnode(-2.5,2){phii}
	\pnode(2.5,2){phij}

	\ncline[arrows=->]{phiQ'}{Qj}
	\ncline[arrows=->]{phiR'}{Rj}
	\ncline[arrows=->]{Qj}{psi}
	\ncline[arrows=->]{Rj}{psi}


	\ncline[arrows=->]{phiQi}{Qi}
	\ncline[arrows=->]{phiRi}{Ri}
	\ncline[arrows=->]{Qi}{psi}
	\ncline[arrows=->]{Ri}{psi}

	\ncline[arrows=-]{phiQi}{phii}
	\ncline[arrows=-]{phiRi}{phii}
	\ncline[arrows=->]{phii}{psi}

	\ncline[arrows=-]{phiQ'}{phij}
	\ncline[arrows=-]{phiR'}{phij}
	\ncline[arrows=->]{phij}{psi}

	\ncarc{Qj}{Qi}
	\ncarc{Rj}{Ri}
	\nccircle[angleA=0]{Qj}{0.5}
	\nccircle[angleA=0]{Qi}{0.5}
	\nccircle[angleA=180]{Rj}{0.5}
	\nccircle[angleA=180]{Ri}{0.5}

	\end{pspicture}%
% NB: X is irrelevant if we are doing an expectation integral -- just drop it
\caption{Bayesian network for marginalising hyperparameters using Bayesian Monte Carlo. Shaded nodes are known and double-circled nodes are deterministic given all their parents. All $Q$ nodes are correlated with one another, as are all $R$ nodes, and the
    context $I$ is correlated with all nodes.}
\label{fig:BMC2}
\end{figure}


Finally, using the further definition for $i,j \in \mathcal{I}_s$, that
\begin{equation}
\Nt(i,j) \defequal \N{\begin{bmatrix} \vph_i \\ \vph_j \end{bmatrix}}{\begin{bmatrix} \vect{\nu} \\ \vect{\nu} \end{bmatrix}}{\begin{bmatrix}  \mat{\lambda}^\tr \mat{\lambda}+\mat{w}^\tr \mat{w} & \mat{\lambda}^\tr \mat{\lambda} \\ \mat{\lambda}^\tr \mat{\lambda} & \mat{\lambda}^\tr \mat{\lambda}+\mat{w}^\tr \mat{w}
\end{bmatrix}}\,,
\end{equation}
our approximation gives us
\begin{equation} \label{eq:muPsi}
\p{\vyst}{\vyD,I} \simeq \vect{q}_s^\tr\,\vect{\rho}\,,
\end{equation}
where the weights of this linear combination are
\begin{equation} \label{eq:weights}
\vect{\rho}\defequal\frac{\mat{K}(\vph_s,\vph_s)^{-1}\,\Nt\,\mat{K}(\vph_s,\vph_s)^{-1}\,\vect{r}_s}
{\ones{s,1}^\tr\,\mat{K}(\vph_s,\vph_s)^{-1}\,\Nt\,\mat{K}(\vph_s,\vph_s)^{-1}\,\vect{r}_s}\,,
\end{equation}
and $\ones{s,1}$ is a vector containing only ones of dimensions equal to $\vect{q}_s$. With a GP on 
$\p{\vyst}{\phi,\,I}$, each $q_i = \p{\vyst}{\vzD,\,\phi_i,\,I}$ will be a slightly different Gaussian. Hence we effectively approximate $\p{\vyst}{\vzD,I}$ as a Gaussian (process) mixture; Bayesian Monte Carlo returns a weighted sum of our predictions evaluated at a sample set of hyperparameters. The assignment of these weights is informed by the best use of all pertinent information. As such, it avoids the risk of overfitting that occurs when applying a less principled technique such as maximum likelihood \cite{MKBook}. 

\subsection{Censored Observations}\label{sec:censored}

In the work above, we have assumed that observations of our variables of interest were corrupted by simple Gaussian noise. However, in many contexts, we instead observe \emph{censored} observations. That is, we might observe that a variable was above or below certain thresholds, but no more. Examples are rich within the weather sensor networks considered. Float sensors are prone to becoming lodged on sensor posts, reporting only that the water level is below that at which it is stuck. Conversely, wind sensors may reach their maximum reading in very strong winds, reporting that the wind is greater than this limit, but providing no more information. Furthermore, readings are often rounded to the nearest integer, meaning that if we observe a reading of $x$, we can say only that the true value was between $x-0.5$ and $x+0.5$. We can readily extend our inference framework to allow for such a noise model. 

More precisely, we assume that we actually observe bounds $\vect{b}_c$ that constrain Gaussian-noise corrupted versions $\vz_c$ of the underlying variables of interest $\vy_c$ at $\vx_c$. This framework allows for imprecise censored observations. Note that the noise variance for censored observations may differ from the noise variance associated with other observations. Conditioned on a combination of censored and un-censored observations, the distribution for our variables of interest is
\begin{align*} %\label{eq:CensoredFull}
& \p{\vyst}{\vzD,\vect{b}_c,I}=\nonumber\\
& \frac{\int\ud\phi\int_{\vect{b}_c}\ud\vz_c\, \p{\vyst}{\vzD,\vz_c,\phi,I}\p{\vz_c}{\vzD,\phi,I}\p{\vzD}{\phi,I}\p{\phi}{I}}
{\int\ud\phi\int_{\vect{b}_c}\ud\vz_c\, \p{\vz_c}{\vzD,\phi,I}\p{\vzD}{\phi,I}\p{\phi}{I}}\,.
\end{align*}
While we cannot determine this full, non-Gaussian distribution easily, we can determine the analytic forms for its mean and covariance. We use the abbreviations $\vect{m}_{c|d,\phi}\defequal\meanskinnysub{\vz}{c}{\vzD,\phi,I}$ and $\mat{C}_{c|d,\phi}\defequal\covskinnysub{\vz}{c}{\vzD,\phi,I}$.
%and abuse notation slightly to have $\Ph{\vect{b}_c}{\vect{m}_{c|d}}{\mat{C}_{c|d}}\defequal\int_{\vect{b}_c}\ud\vz_c\,\N{\vz_c}{\vect{m}_{c|d}}{\mat{C}_{c|d}}$. 
To reflect the influence of our censored observations, our likelihoods become
\begin{align*}
 \p{\vzD,\,\vect{b}_c}{\phi,\,I}
  &= \N{\vzD}{\meanskinnysub{\vz}{d}{\phi,I}}{\covskinnysub{\vz}{d}{\phi,I}}\int_{\vect{b}_c}\ud\vz_c\,\N{\vz_c}{\vect{m}_{c|d,\phi}}{\mat{C}_{c|d,\phi}}\,,
\end{align*}
giving the new weights over hyperparameter samples $\vect{\rho}^{(cd)}$.
%$\vect{r}^{(cd)}_s\defequal r^{(cd)}(\vphS)$
We can then write our predictive mean as
% \begin{align*}
% & \meansub{\vy}{\star}{\vzD,\vect{b}_c,I}=\nonumber\\
% & \frac{\int\ud\phi\int_{\vect{b}_c}\ud\vz_c\, \mean{\vyst}{\vzD,\,\vz_c,\,\phi,\,I}\p{\vz_c}{\vzD,\,\phi,\,I}\p{\vzD}{\phi,\,I}\p{\phi}{I}}
% {\int\ud\phi\int_{\vect{b}_c}\ud\vz_c\, \p{\vz_c}{\vzD,\,\phi,\,I}\p{\vzD}{\phi,\,I}\p{\phi}{I}}\,.
% \end{align*}
\begin{multline*} %\label{eq:CensoredMean}
\meansub{\vy}{\star}{\vzD,\vect{b}_c,I}=\sum_{i\in s}\rho^{(cd)}_i\Biggl(\vect{\mu}(\vxst;\phi_i)+\mat{K}(\vxst,[\vx_c,\vxD];\phi_i)\\\mat{V}([\vx_c,\vxD],[\vx_c,\vxD];\phi_i)^{-1}
\Biggl[\begin{array}{r}
	\frac{\int_{\vect{b}_c}\ud\vz_c\,\vz_c\, \N{\vz_c}{\vect{m}_{c|d,\phi_i}}{\mat{C}_{c|d,\phi_i}}}{\int_{\vect{b}_c}\ud\vz_c\,\N{\vz_c}{\vect{m}_{c|d,\phi_i}}{\mat{C}_{c|d,\phi_i}}}-\vect{\mu}(\vy_c;\phi_i)\\
       \vzD-\vect{\mu}(\vxD;\phi_i)
      \end{array}\Biggr]\Biggr)\,,
\end{multline*}
noting that a censored observation is intuitively treated as an uncensored observation equal to the conditional mean of the GP over the bounded region. We have also the predictive covariance
\begin{multline*} %\label{eq:CensoredCov}
\covsub{\vy}{st}{\vzD,\vect{b}_c,I}=\sum_{i\in s}\rho^{(cd)}_i \Big(\mat{K}(\vxst,\vxst;\phi_i)-\\\mat{K}(\vxst,[\vx_c,\vxD];\phi_i)\mat{V}([\vx_c,\vxD],[\vx_c,\vxD];\phi_i)^{-1}\mat{K}([\vx_c,\vxD],\vyst;\phi_i)\Big)\,.
\end{multline*}

We now have the problem of determining the integrals 
\begin{equation*} %\label{eq:censored_bad_integrals}
\int_{\vect{b}_c}\ud\vz_c\,\N{\vz_c}{\vect{m}_{c|d,\phi}}{\mat{C}_{c|d,\phi}} \qquad \text{and} \qquad
\int_{\vect{b}_c}\ud\vz_c\,\vz_c\, \N{\vz_c}{\vect{m}_{c|d,\phi}}{\mat{C}_{c|d,\phi}}\,,
\end{equation*}
which are non-analytic. With only a single censored observation, we can use standard functions for such integrals. Assuming that our censored observations are independent of one another (conditioned on the other observations $\vzD$) would also allow us the use of such results, but is usually an unreasonable approximation. Fortunately, there also exists an efficient Monte Carlo algorithm \cite{genz1992ncm} for the purpose of resolving such integrals. It is this algorithm that we will choose for the appropriate empirical tests to follow.

\subsection{Efficient Implementation}\label{sec_efficient}

\noindent Now the implementation of equations \eqref{eq:GPMean} and \eqref{eq:GPVar} involves the inverse of a matrix, and the most stable way to implement this is through the use of the Cholesky decomposition, $\mat{R}(\vxD,\vxD)$, of $\mat{V}(\vxD,\vxD)$, rather than inverting the matrix directly. %, such that $\mat{V}(\vxD,\vxD)=\mat{R}(\vxD,\vxD)^\tr\,\mat{R}(\vxD,\vxD)$. %We denote the Cholesky operation as $\mat{R}(\vzD,\vzD)=\chol(\mat{V}(\vzD,\vzD))$. 
%This upper triangular factor can then be used to efficiently solve our required triangular sets of linear equations.
Performing this Cholesky decomposition represents the most computationally expensive operation we must perform; its cost scaling as $O(N^3)$ in the number of data points $N$. However, as discussed earlier, we do not intend to use our GP with a fixed set of data, but rather, within an on-line algorithm that receives new observations over time. As such, we must be able to iteratively update our predictions in as little time as possible. Fortunately, we can do so by exploiting the special structure of our problem. When we receive new data, our $\mat{V}$ matrix is changed only in the addition of a few new rows and columns. Hence most of the work that went into computing its Cholesky decomposition at the last iteration can be recycled to produce the new Cholesky decomposition (see Appendix \ref{sec:CholeskyUpdate} for details of this operation). Another problematic calculation required by \eqref{eq:GPMean} and \eqref{eq:GPVar} is the computation of the data-dependent term $\mat{R}(\vxD,\vxD)^{-1}(\vyD-\vect{\mu}(\vxD))$, in which $\vyD-\vect{\mu}(\vxD)$ is also only changing due to the addition of new rows. As such, efficient updating rules are also available for this term (see Appendix \ref{sec:DataTermUpdate}). Finally, we are also able to perform an efficient update to the likelihood\footnote{Note that likelihoods are commonly smaller than machine precision and so for numerical reasons we prefer to instead work with log-likelihoods, exponentiating as required.} of our hyperparameters, as per Appendix \ref{sec:LogGaussianUpdate}. As per \eqref{eq:weights}, the updated sample likelihoods $\vect{r}_s$ then give rise to updated weights $\rho$ over our hyperparameter samples. With these efficient rules, we are able to reduce the overall cost of an update from $O(N^3)$ to $O(N^2)$.

However, we can further increase the efficiency of our updates by making a judicious assumption. In particular, experience shows that our GP requires only a very small number of recent observations in order to produce good estimates. Indeed, most covariance functions have very light tails such that only points within a few multiples of the time scale are at all relevant to the point of interest. Hence we seek sensible ways of discarding information once it has been rendered `stale', to reduce both memory usage and computational requirements.

One pre-eminently reasonable measure of the value of data is the uncertainty we still possess after learning it. In particular, we are interested in how uncertain we are about $\vxst$; as given by the covariance of our Gaussian mixture equation \eqref{eq:muPsi}. Our approach is thus to drop our oldest data points (those which our covariances typically deem least relevant to the current predictant) until this uncertainty exceeds some predetermined threshold. 

Just as we were able to efficiently update our Cholesky factor upon the receipt of new data, so we can downdate to remove data (see Appendix \ref{sec:CholeskyDowndate} for the details of this operation). Unfortunately, there is no such efficient downdate for the data-dependent term $\mat{R}(\vxD,\vxD)^{-1}(\vyD-\vect{\mu}(\vxD))$. Naturally there is no computational benefit here to be gained in downdating our hyperparameter likelihoods, whose dimension remains constant regardless of the size of the dataset. Hence our likelihoods are never downdated, and we retain all knowledge of otherwise dropped data that is pertinent to our marginalisation of hyperparameters. 

Our proposal allows us to rapidly remove unwanted data, compute our uncertainty about $\vyst$, and then repeat as required; the GP will retain only as much data as necessary to achieve a pre-specified degree of accuracy. This allows a principled way of `windowing' our data series.

Finally, we turn to the implementation of our marginalisation procedure. Essentially, our approach is to maintain an ensemble of GPs, one for each hyperparameter sample, each of which we update and downdate according to the proposals above. Note that the computations for each hyperparameter sample can be treated independently, lending our algorithm to parallelisation. 

The predictions of each sample are then weighted and combined according to equation \eqref{eq:muPsi}. Note that the only computations whose computational cost grows at greater than a quadratic rate in the number of samples, $\eta$, are the Cholesky decomposition and multiplication of covariance matrices in equation \eqref{eq:weights}, and these scale rather poorly as $O(\eta^3)$. To address this problem, we take our Gaussian priors for each different hyperparameter $\phi_{(e)} \in \phi$ as independent. We further take a covariance structure given by the product of terms over each hyperparameter, the common \emph{product correlation rule} (e.g. \citeN{Sasena})
\begin{equation}
 K(\phi,\,\phi')=\prod_e K_e\Big(\phi_{(e)},\,\phi'_{(e)}\Big)\,.
\end{equation}
If we additionally consider a simple grid of samples, such that $\vph_s$ is the tensor product of a set of samples $\vph_{(e),s}$ over each hyperparameter, then the problematic term in equation \eqref{eq:muPsi} reduces to the Kronecker product of the equivalent term over each individual hyperparameter:
\begin{align}\label{eq:KNK}
\mat{K} \!(\vph_s,\vph_s)^{-1}\,\Nt\,&\mat{K}\!(\vph_s,\vph_s)^{-1}\nonumber\\
 = & ~\mat{K}\!(\vph_{(1),s},\vph_{(1),s})^{-1}\Nt\!(\vph_{(1),s},\vph_{(1),s})\mat{K}\!(\vph_{(1),s},\vph_{(1),s})^{-1}\nonumber\\ & \otimes \mat{K}\!(\vph_{(2),s},\vph_{(2),s})^{-1}\Nt\!(\vph_{(2),s},\vph_{(2),s})\mat{K}\!(\vph_{(2),s},\vph_{(2),s})^{-1}\nonumber\\ & \otimes \ldots
\end{align}
This means that we only have to perform the expensive Cholesky factorisation and multiplication with matrices whose size equals the number of samples for each hyperparameter, rather than on a matrix of size equal to the total number of hyperparameter samples. This hence represents an effective way to avoid the `curse of dimensionality'. 

Note that we can also leverage some of the benefits of maximum likelihood to our own advantage. In particular, given an initial grid of samples, we can perform regular updates to their positions by performing single-stage gradient ascent. That is, the value of $\phi_{(e),i}$, the $i$th sample in the $e$th hyperparameter, is adjusted according to the average derivative with respect to $\phi_{(e),i}$ of the likelihood of all samples that share $\phi_{(e),i}$. Shifting a hyperparameter sample does, however, require the fresh Cholesky decomposition of the covariance matrix associated with that sample, along with the re-computation of the data-dependent term, likelihood and the weights term \eqref{eq:KNK}. As such, we perform such sample shifting relatively rarely to avoid too excessive a computational burden.

In general, limiting the number of hyperparameter samples is of critical importance to achieving practical computation. As such, we should exploit any and all prior information that we possess about the system to limit the volume of hyperparameter space that our GP is required to explore online. For example, an informative prior expressing that a tidal period is likely to be around half a day will greatly reduce the number of samples required for this hyperparameter. Similarly, an offline analysis of any available training data will return sharply peaked posteriors over our hyperparameters that will further restrict the required volume to be searched over on-line. For example, we represent the tidal period hyperparameter with only a single sample on-line, so certain does training data make us of its value. Finally, a simpler and less flexible covariance model, with fewer hyperparameters, could be chosen if computational limitations become particularly severe. 

Applied together, these features provide us with an efficient on-line algorithm that can be applied in real-time as data is sequentially collected from the sensor network. Pseudo-code for our tracking algorithm is displayed in algorithm \ref{alg:track}.


\begin{algorithm}[htp]
  \caption{{\sc GP-Track}$(\vx_{1:T}, \vz_{1:T}, \delta, $ {\sc Mean}$(\cdot;\cdot), $ {\sc Cov}$(\cdot,\cdot;\cdot), \vect{\phi}_s)$}
\cmmnt Returns the sequential lookahead predictions made with our GP algorithm.\\
\begin{tabular}{l r l}
\cmmnt & $\{(x_t, z_t)\}_{t=1}^T$: & All data.\\
\cmmnt & $\delta$: & Lookahead.\\
\cmmnt & {\sc Mean}$(x,\phi)$: & Mean function ($\mu$), taking as arguments an input $x$ and \\
\cmmnt & &  hyperparameter vector $\phi$. \\
\cmmnt & {\sc Cov}$(x,x';\phi)$: & Covariance function ($K$), taking as arguments two inputs\\
\cmmnt & &  $x$ and $x'$ and hyperparameter vector $\phi$. \\
\cmmnt & $\vect{\phi}_s\defequal\{{\phi}_j\}_{i=1}^\eta$: & Hyperparameter samples. \\
\end{tabular}
  \begin{algorithmic}[1]
\STATE $\vect{x}_d \leftarrow \{\}$
\STATE $\vect{z}_d \leftarrow \{\}$
\FOR{$i=1$ {\bfseries to} $\eta$}
\STATE $GP[i] \leftarrow$ {\sc Initialise-GP}$\bigl(${\sc Mean}, {\sc Cov}, $\phi_i\bigr)$ \cmmnt Supply $GP[i]$ with the mean function {\sc Mean}$(x,\phi_i)$ and covariance function {\sc Cov}$(x,x';\phi_i)$.
\ENDFOR
\STATE {\it weights-term} $\leftarrow$ {\sc Calculate-Weights-Term}$\bigl(\vect{\phi}_s\bigr)$ \cmmnt Calculate the weights term from \eqref{eq:KNK}, 
$\mat{K} \!(\vph_s,\vph_s)^{-1}\,\Nt\,\mat{K}\!(\vph_s,\vph_s)^{-1}$.
    \FOR{$t=1$ {\bfseries to} $T - \delta$}
\STATE $(\vect{x}_d, \vect{z}_d, $ {\it drop-inds, add-ind}$)  \leftarrow$ {\sc Increment-Data}$\bigl((\vect{x}_d, \vect{z}_d), (x_t, z_t),GP\bigr) $ \\\cmmnt Add new datum (index {\it add-ind}), and drop old data (indices {\it drop-inds}) as necessary (as per Section \ref{sec_efficient}).
    \FOR{$i=1$ {\bfseries to} $\eta$}
%     \STATE {\it model}$[j]\!\leftarrow\!$ {\sc update-model}$(${\it model}$[j], \vect{x}_t, 
% z_t)$
\STATE $GP[i] \leftarrow$ {\sc Downdate-GP}$\bigl(GP[i],${\it drop-inds}$\bigr)$ \cmmnt Downdates a GP as per Section \ref{sec_efficient}, revising covariance matrix and data-dependent term to allow for dropped data.
\STATE $GP[i] \leftarrow$ {\sc Update-GP}$\bigl(GP[i], (\vect{x}_d, \vect{z}_d), ${\it add-ind}$\bigr)$ \cmmnt Updates a GP as per Section \ref{sec_efficient}, revising covariance matrix, data-dependent term and likelihoods to allow for added data.
\STATE $(m_{t+\delta,i},C_{t+\delta,i}) \leftarrow$ {\sc Make-Predictions}$\bigl(x_{t+\delta},GP[i]\bigr)$ \cmmnt Compute the posterior mean and variances for predictant $\vyst = y_{t+\delta}$ using \eqref{eq:GPMean} and \eqref{eq:GPVar}.
    \ENDFOR
    \STATE $\vect{\rho} \leftarrow$ {\sc Determine-Weights}$(${\it weights-term}$,GP[1:\eta])$ \cmmnt Using \eqref{eq:weights}.
    \STATE $(m'_{t+\delta},C'_{t+\delta}) \leftarrow$ {\sc Combine-Predictions}$(\{\rho_i,m_{t+\delta,i},C_{t+\delta,i}\}_{i=1}^{\eta})$ \\\cmmnt Gaussians are combined in a weighted mixture, using \eqref{eq:muPsi}.
    \STATE {\bf Return:} $(m'_{t+\delta},C'_{t+\delta})$
\IF{{\sc Is-Time-To-Move}(t) \cmmnt Infrequently true} 
    \STATE $(\vect{\phi}_s, $ {\it move-inds}$) \leftarrow$ {\sc Manage-Hypersamples}$( 
GP[1:\eta], \vect{\phi}_s, t)$\\ \cmmnt Move hyperparameter samples {\it move-inds}, as per Section \ref{sec_efficient}.
\FOR{$i\in $ {\it move-inds}}
\STATE $GP[i] \leftarrow$ {\sc Overwrite-GP}$\bigl(GP[i], (\vect{x}_d, \vect{z}_d), \phi_i\bigr)$ \cmmnt Recompute all quantities given the new hyperperparameter sample.
\ENDFOR
\STATE {\it weights-term} $ \leftarrow$ {\sc Calculate-Weights-Term}$\bigl(\vect{\phi}_s\bigr)$
\ENDIF
    \ENDFOR
  \end{algorithmic}
  \label{alg:track}
\end{algorithm}

\subsection{Active Data Selection}

\noindent Finally, in addition to the regression and prediction problem described in Section \ref{sec_info}, we are able to use the same algorithm to perform active data selection. This is a decision problem concerning which observations should be taken. In this, we once again take a utility that is a function of the uncertainty in our predictions. We specify a utility of negative infinity if our uncertainty about any variable is greater than a pre-specified threshold, and a fixed negative utility is assigned as the cost of an observation (in general, this cost could be different for different sensors). Note that the uncertainty increases monotonically in the absence of new data, and shrinks in the presence of an observation. Hence our algorithm is simply induced to make a reading whenever the uncertainty grows beyond a pre-specified threshold. 

Our algorithm can also decide which observation to make at this time, by determining which sensor will allow it the longest period of grace until it would be forced to observe again. This clearly minimises the number of costly observations. Note that the uncertainty of a GP, as given by \eqref{eq:GPVar}, is actually dependent only on the location of the observation, not its actual value. Hence the uncertainty we imagine remaining after taking an observation from a sensor can be quickly determined without having to speculate about what data we might possibly collect. However, this is true only so long as we do not consider the impact of new observations on our hyperparameter sample weights \eqref{eq:weights}. Our approach is to take the model, in particular the weights over samples, as fixed, and investigate only how different schedules of observations affect our predictions within it. With this proviso, we are guaranteed to maintain our uncertainty below a specified threshold, while taking as few observations as possible.

\section{Trial Implementation}\label{sec_implementation}

\begin{figure}
\begin{center}
\epsfig{figure=figures/bramble.eps,width=2.64cm} \hspace{0.25cm}
\epsfig{figure=figures/bramble_web.eps,width=6.0cm}
\caption{The Bramble Bank weather station and web site.}
\label{bramble_sensor}
\end{center}
\end{figure}

\noindent In order to empirically evaluate the information processing algorithm described in the previous section, we have used data from three networks of weather sensors of different sizes (specifically, 4, 16 and 670 sensors) that provide a range of different sensor measurements. These networks are the Bramblemet sensor network, the Wannengrat Alpine Observatory, and the UK Met Office MIDAS land surface weather stations. The use of these weather sensor networks is attractive since they exhibit challenging correlations and delays whose physical processes are well understood. However, we stress that the approach that we describe in this paper is general, and can equally well be applied to any time series that exhibits complex correlations and delays.

\subsection{Bramblemet Weather Sensor Network}

\noindent The Bramblemet network consists of four sensors (named Bramblemet, Sotonmet, Cambermet and Chimet), each of which measures a range of environmental variables (including wind speed and direction, air temperature, sea temperature, and tide height) and makes up-to-date sensor measurements available through separate web pages (see figure \ref{bramble_sensor} and \small\url{http://www.bramblemet.co.uk/}\normalsize). To facilitate the autonomous collection of sensor data by our information processing algorithm, we have worked with the sensor network operators (Associated British Ports) and supplemented each sensor web page with machine readable RDF data (see figure \ref{rdf} for an example of this format -- current sensor data in this format is available at \small\url{http://www.bramblemet.co.uk/bra.rdf}\normalsize). This format is attractive as it represents a fundamental element of the semantic web, and there exist a number of software tools to parse, store and query it. More importantly, it allows the sensor data to be precisely defined through standard ontologies \cite{rdf}. For example, linking the predicate {\em geo:lat} to the ontology available at \small\url{http://www.w3.org/2003/01/geo/wgs84_pos#}\normalsize ~precisely defines the value {\em ``50.79472''} as representing a latitude in the WGS84 geodetic reference datum. While ontologies for sensor data have yet to be standardised, a number of candidates exist (see \cite{sensor_rdf} for an example based upon Sensor Web Enablement (SWE) and SensorML data component models).

\begin{figure}
\centering \rule{3.3in}{.005in}\par
\begin{minipage}{2.6in}
\renewcommand{\baselinestretch}{1.5}
\begin{tabbing} \\[-3.2ex]
llll \=llll \=llll \=llll    \kill
\\
\tt\scriptsize \ <\color{red}sit:Location\color{black}~rdf:about=\color{blue}"\&sit;bramblemet"\color{black}\\
\tt\scriptsize \ \ \ rdfs:label=\color{blue}"Bramble Bank"\color{black}\\
\tt\scriptsize \ \ \ geo:lat=\color{blue}"50.79472"\color{black}\\
\tt\scriptsize \ \ \ geo:lng=\color{blue}"-1.2875"\color{black}\\
\tt\scriptsize \ \ \ sit:altitude=\color{blue}"1"\color{black}>\\
\tt\scriptsize \ </\color{red}sit:Location\color{black}>\\
\\
\tt\scriptsize \ <\color{red}sit:Sensor\color{black}~rdf:about=\color{blue}"\&sit;bramblemet/windspeed"\color{black}\\
\tt\scriptsize \ \ \ rdfs:label=\color{blue}"Wind speed"\color{black}>\\
\tt\scriptsize \ \ \ <\color{red}sit:sensorType\color{black}~rdf:resource=\color{blue}"\&sit;windspeed"\color{black}/>\\
\tt\scriptsize \ \ \ <\color{red}sit:location\color{black}~rdf:resource=\color{blue}"\&sit;bramblemet"\color{black}/>\\
\tt\scriptsize \ </\color{red}sit:Sensor\color{black}>\\
\\
\tt\scriptsize \ <\color{red}sit:SensorType\color{black}~rdf:about=\color{blue}"\&sit;windspeed"\color{black}\\
\tt\scriptsize \ \ \ rdfs:label=\color{blue}"Wind speed"\color{black}>\\
\tt\scriptsize \ </\color{red}sit:SensorType\color{black}>\\
\\
\tt\scriptsize \ <\color{red}sit:Unit\color{black}~rdf:about=\color{blue}"\&sit;knots"\color{black}\\
\tt\scriptsize \ \ \ rdfs:label=\color{blue}"Knots"\color{black}\\
\tt\scriptsize \ \ \ sit:unitAbbr=\color{blue}"kn"\color{black}>\\
\tt\scriptsize \ </\color{red}sit:Unit\color{black}>\\
\\
\tt\scriptsize \ <\color{red}sit:Reading\color{black}\\
\tt\scriptsize \ \ \ rdf:about=\color{blue}"\&sit;bramblemet/windspeed/reading/1234"\color{black}\\
\tt\scriptsize \ \ \ rdfs:value=\color{blue}"9.3"\color{black}\\
\tt\scriptsize \ \ \ sit:datetime=\color{blue}"2007-10-25T21:55:00"\color{black}>\\
\tt\scriptsize \ \ \ <\color{red}sit:sensor\color{black}~rdf:resource=\color{blue}"\&sit;bramblemet/windspeed"\color{black}/>\\
\tt\scriptsize \ \ \ <\color{red}sit:unit\color{black}~rdf:resource=\color{blue}"\&sit;knots"\color{black}/>\\
\tt\scriptsize \ </\color{red}sit:Reading\color{black}>\\
\normalsize
\end{tabbing}
\end{minipage}\par
\rule{3.3in}{.005in} 
\caption{Example RDF data from the Bramblemet sensor.}
\label{rdf}
\end{figure}

In order to visualise the sensor data and the information processing algorithms in operation, we have implemented a server-based application that collects the RDF data from the sensors, parses and stores it using Jena (see \url{http://jena.sourceforge.net/}), and displays sensor data in tabular and graphical forms using a Google Maps interface (see figure \ref{screen}). A live version of this system is available at \url{http://www.aladdinproject.org/situation/}.

\begin{figure}[htp]
\begin{center}
\epsfig{figure=figures/screen.eps,width=10.5cm}
\caption{Server-based implementation of our information processing algorithm accessed through a web based Google Maps interface (available at \tt{http://www.aladdinproject.org/situation/}).}
\label{screen}
\end{center}
\end{figure}

The use of the Bramblemet sensor network is particularly attractive since the network is subject to real network outages that generate instances of missing sensor readings on which we can evaluate our information processing algorithms. Furthermore, by working with the sensor network operators we have been able to recover the missing data from these periods from the sensors' local caches, and thus, perform quantitative comparisons between our information processing algorithm's predictions and the actual data. Furthermore, within the Bramblemet network sensors, the sensor measurements are rounded prior to their transmission over the wireless link. This results in censored observations, and thus, again by accessing the raw data from the sensors' local caches, we are also able to compare the ability of our information processing algorithms to handle these censored observations.

However, the Bramblemet sensor network only consists of four sensors, and thus, in order to compare the performance of our approach on larger networks, we turn to the Wannengrat Alpine observatory and the UK Met Office MIDAS land surface stations.

\subsection{Wannengrat Alpine Observatory}

The Wannengrat site consists of over twenty SensorScope stations to measure local weather conditions\footnote{see \url{http://www.swiss-experiment.ch/index.php/Wannengrat:Sensorscopedeployment}.}. This wireless sensor network is deployed with the aim of understanding the complex meterological processes occurring on the snowpack. For this network, we simply use the data from online data repository, and have had no direct interaction with the sensor network operators.

\subsection{MIDAS Land Surface Weather Stations}

Finally, we use data from the UK Met Office MIDAS land surface stations data \cite{MIDASdata}, focusing on the network's readings of maximum daily temperature at 670 sensors distributed across the UK. Our experiments on the data collected from these several hundred sensors serve as a demonstration of the ability of our algorithm to cope with larger sensor networks. 

\section{Empirical Evaluation}\label{sec_evaluation}

\noindent In this section we empirically evaluate our information processing algorithm on real weather data collected from the sensor networks described above. 
We will compare the use of our GP formalism built around a multiple-sensor covariance against a number of benchmarks:
\begin{itemize}
\item[\bf Conventional independent GP\normalfont] in which each environmental variable is modelled separately (i.e. correlations between these parameters are ignored). 
\item[\bf Kalman filter\normalfont] in which a maximum a posteriori
(MAP) recursion is used to govern the adaptions of the hyper-parameters (namely the state and observation noise processes) by sequentially using the maximum-likelihood formulation first proposed by \citeN{Jazwinski}. One step-ahead forecasts are successively iterated through the algorithm to provide multi-step
forecasts of arbitrary length. Missing values, if they occur, are
accommodated for using the scheme advocated by 
\citeN{Shumway+Stoffer:00}.  The algorithm operates, for multiple time-series
forecasting, using a $p$-th order lookback into the data, including
cross-coupling between time-series, making it a recursive multivariate
autoregressive, MAR($p$), process.
\item[\bf Na\"{i}ve algorithm\normalfont] in which the prediction of the variable at the next time step is simply equal to that of the most recent observation at that sensor.
\end{itemize}
The first benchmark illustrates the effectiveness of our Gaussian process formalism that expresses correlations between different sensors. The second benchmark is a state-of-the-art alternative, that has previously been used for tracking environmental sensor data \cite{kalman_oceanography}. Finally, the na\"{i}ve third benchmark provides a lower bound on the performance of a prediction algorithm.

For the purposes of illustration, we will plot the sensor readings acquired by our algorithm (shown as markers), the mean and standard deviation of the GP prediction (shown as a solid line with plus or minus a single standard deviation shown as shading), and the true fine-grained sensor readings (shown as bold) that were downloaded directly from the sensor (rather than through the web site) after the event. Note that we present a limited number of sensors for reasons of space, but we use readings from all relevant sensors in order to perform inference. 

Where appropriate, predictive quality will be evaluated using the standard root mean squared error (RMSE), defined as
\begin{equation*}
 \text{RMSE}(\vect{m},\vy) = \sqrt{\frac{1}{N}\sum_{i=1}^N (m_i-y_i)^2}\,.
\end{equation*}
Here $\vect{m}$ is a vector of $N$ estimates for the vector of variables $\vy$. The estimates we use are the means for $\vy$ conditioned on some set of observations $\vz$ (as dependent on the application). We also provide the normed mean squared errors (NMSEs), defined as
\begin{equation*}
 \text{NMSE}(\vect{m},\vy) = 10 \log_{10} \frac{\nicefrac{1}{N} \sum_{i= 1}^{N} (m_i - y_i)^2 }{\nicefrac{1}{N} \left(\sum_{i = 1}^{N} y_i^2\right) - \left(\nicefrac{1}{N} \sum_{i = 1}^{N} y_i\right)^2 }\,,
\end{equation*}
which have decibels as units.

It is commonly the case that after a significant volume of data, the majority of the weight will be concentrated at a single trial hyperparameter sample -- the posterior distribution for the hyperparameter $\p{\phi}{\vzD,\,I}$ will be close to a delta function at that value. In this case, we will say, informally, that we have \emph{learned} the hyperparameter to have that single value. Where this appears to be the case, performing a local optimisation of the likelihood $\p{\vzD}{\phi,\,I}$ can help to find the point of highest posterior density.

In the subsections that follow we first evaluate our approach on tide height, air temperature and rounded air pressure data from the Bramblemet weather sensor network, and then discuss the prediction of maximum daily air temperature over the larger MIDAS land surface weather station network. In each case, we describe the covariance function used, and present quantitative comparisons of prediction quality. We then go on to discuss our approach to active data selection using the tide and wind speed sensors of the Bramblemet weather sensor network, and the air temperature sensors of the Wannangrat Alpine Observatory.

\subsection{Prediction of Tide Heights} \label{sec:tide_pred}

\begin{figure}
\begin{center}
\begin{tabular}{cc}
\hspace{-0.75cm}\epsfig{figure=figures/indep_tide_1_reg.eps,width=7.2cm} & \hspace{-1.00cm}\epsfig{figure=figures/dep_tide_1_reg.eps,width=7.2cm} \\
\hspace{-0.75cm}\epsfig{figure=figures/indep_tide_3_reg.eps,width=7.2cm} & \hspace{-1.00cm}\epsfig{figure=figures/dep_tide_3_reg.eps,width=7.2cm} \\
\hspace{-0.6cm}(a) & \hspace{-0.6cm}(b) \\
\end{tabular}
\caption{Prediction and regression of tide height data for (a) independent and (b) multi-output Gaussian processes.}
\label{fig:tide_reg}
\end{center}
\end{figure}

Figure \ref{fig:tide_reg} illustrates the efficacy our GP prediction for a tide height dataset. That is, at time $t$, figure \ref{fig:tide_reg} depicts the posterior distribution of the GP, conditioned on all observations prior to and inclusive of $t$. 
In order to manage the four outputs of our tide function (one for each sensor), we rewrite so that we have a single output and inputs $t$, time, and $l$, a sensor label. As such, our covariance function is of the form
\begin{multline} \label{eq:covtides}
 K\bigl([t,\,l],\,[t',\,l']\bigr)={W}(l,\,l')\,\Bigl(
\Kl{per-Mtn}\bigl(t-\Delta_{l},t'-\Delta_{l'};h\dnt{P}=1,\,w\dnt{P},\nu=\tfrac{5}{2}\bigr)+
\\
\Kl{Mtn}\bigl(t-\Delta_{l},t'-\Delta_{l'};h\dnt{D},\,w\dnt{D},\nu=\tfrac{5}{2}\bigr)
\Bigr)\,,
\end{multline}
where $W$ is a covariance matrix parameterised using the spherical parameterisation \eqref{eq:spherical}. We used two different GP models: one in which the parameters of the spherical decomposition were inferred from the data; and one for which the matrix $S$ was taken as an identity matrix. In the former case, we consequently have a multi-output formalism, in the latter, independent GPs for each sensor.

 Note that our covariance over time is the sum of a periodic term and a \emph{disturbance} term. Both are of the Mat\'{e}rn form with 
$\nu=\frac{5}{2}$, \eqref{eq:permatern} and \eqref{eq:matern} respectively. 
This form is a consequence of our expectation that the tides would be well modelled by the superposition of a simple periodic signal and an occasional disturbance signal due to exceptional conditions. Of course, for a better fit over the course of, say, a year, it would be possible to additionally incorporate longer-term drifts and periods.

The period $w_P$ of the first term was unsurprisingly learnt as being about half a day, whereas for the disturbance term the time scale $w\dnt{D}$ was found to be about two and a half hours. Note that this latter result is concordant with our expectations for the time scales of the weather events we intend our disturbance term to model. 

Our algorithm learned that all four sensors were very strongly correlated, with $S^\tr S$ (where $S$ is from \eqref{eq:spherical})  containing elements all very close to one. $W$ additionally gives an appropriate length scale for each sensor. Over this data set, the Chimet sensor was found to have a length scale of $1.4$m, with the remainder possessing scales of close to $1$m. Note that $h\dnt{P}$ and $h\dnt{D}$ in \eqref{eq:covtides} are dimensionless ratios, serving as weightings between our various covariance terms. Hence we can set $h\dnt{D}=1$ without loss of generality, upon which we learn that $h\dnt{P}$ is around $0.2$. Hence weather events were observed to have induced changes in tide height on the order of $20\%$.

We also make allowances for the prospect of relative latency amongst the sensors by incorporating delay variables, represented by the vector $\Delta$. We found that the tide signals at the Cambermet and Chimet stations were delayed by about $10$ minutes relative to the other two. This makes physical sense -- the Bramblemet and Sotonmet stations are quite exposed to the ocean, while water has to run further through reasonably narrow channels before it reaches the Cambermet and Chimet stations.

Only the correlations amongst sensors and their individual delays were sampled over, all other hyperparameters for our covariance were learned by the GP prior to the depicted data set. Such hyperparameters were then specified with single samples at the learned values.

\begin{table}
\centering
\caption{Predictive performances for five-day Bramblemet tide height dataset.}
\label{tbl:TH_RMSEs}
 \begin{tabular}{@{}llr@{}}
 \\
 \toprule
Algorithm & RMSE (m) & NMSE (dB)\\
\midrule
Na\"{i}ve & 7.5 $\times 10^{-1}$ & -2.1 \\
Kalman filter & 3.9 $\times 10^5$ & 112.2 \\
Independent GPs & 8.7 $\times 10^{-2}$ & -20.3 \\
Multi-output GP & 3.8 $\times 10^{-2}$ & -27.6 \\
\bottomrule
\end{tabular}
\end{table}

Note the performance of our multi-output GP formalism when the Bramblemet sensor drops out at $t=1.45$ days.
In this case, the independent GP quite reasonably predicts that the tide will repeat the same periodic signal it has observed in the past. However, the GP can achieve better results if it is allowed to benefit from the knowledge of the other sensors' readings during this interval of missing data. Thus, in the case of the multi-output GP, by $t=1.45$ days, the GP has successfully determined that the sensors are all very strongly correlated. Hence, when it sees an unexpected low tide in the Chimet sensor data (caused by the strong Northerly wind), these correlations lead it to infer a similarly low tide in the Bramblemet reading. Hence, the multi-output GP produces significantly more accurate predictions during the missing data interval, with associated smaller error bars. Exactly the same effect is seen in the later predictions of the Chimet tide height, where the multi-output GP predictions use observations from the other sensors to better predict the high tide height at $t=2.45$ days. 

Note also that there are two brief intervals of missing data for all sensors just after both of the first two peak tides. During the second interval, the GP's predictions for the tide are notably better than for the first -- the greater quantity of data it has observed allows it to produce more accurate predictions. With time, the GP is able to build successively better models for the series.

The predictive performances for our various algorithms over this dataset can be found in table \ref{tbl:TH_RMSEs}. Note that for the Kalman filter tested, the lookback $p$ was taken as 16. Note that our multi-output GP which  exploits correlations between the sensors, and the periodicity in each individual sensors' measurements, significantly outperforms the Kalman filter and the independent GP.


\subsection{Prediction of Air Temperatures}


\begin{figure}
\begin{center}
\begin{tabular}{cc}
\hspace{-0.75cm}\epsfig{figure=figures/AT_Bramble_GP.eps,width=7.2cm} & \hspace{-1.00cm}\epsfig{figure=figures/AT_Bramble_KF.eps,width=7.2cm} \\
\hspace{-0.75cm}\epsfig{figure=figures/AT_Chi_GP.eps,width=7.2cm} & \hspace{-1.00cm}\epsfig{figure=figures/AT_Chi_KF.eps,width=7.2cm} \\
\hspace{-0.6cm}(a) & \hspace{-0.6cm}(b) \\
\end{tabular}
\caption{One-step lookahead prediction of air temperature data for (a) a multi-output Gaussian process and (b) a Kalman filter.}
\label{at_reg}
\end{center}
\end{figure}

We next tested on air temperature data. Figure \ref{at_reg} demonstrates our algorithm's 5 minute lookahead predictive performance, where 5 minutes was also the usual (although not exclusive) time increment from one datum to the next. That is, figure \ref{at_reg} depicts the posterior distribution at time $t$ conditioned on all observations prior to and inclusive of $t-5\,\text{mins}$. Note that the Cambermet station does not record air temperature, and is hence discluded from consideration here. Taking the covariance function used for tide height data shown in equation \eqref{eq:covtides} as a model, we assumed the covariance function
\begin{multline*} %\label{eq:covAT}
 K\bigl([t,\,l],\,[t',\,l']\bigr)={W}(l,\,l')\\
\Bigl(
\Kl{per-Mtn}\bigl(t-\Delta_{l},t'-\Delta_{l'};h\dnt{P}=1,\,w_P,\nu=\tfrac{5}{2}\bigr)+
\\
\Kl{Mtn}\bigl(t-\Delta_{l},t'-\Delta_{l'};h\dnt{T},\,w\dnt{T},\nu=\tfrac{5}{2}\bigr)
\Bigr)+
\\
\dd{l}{l'}\Kl{Mtn}\bigl(t,t';h\dnt{D},\,w\dnt{D},\nu=\tfrac{3}{2}\bigr)
\,,
\end{multline*}
Our covariance over time consisted of three terms. The first was periodic, whose period $w_P$ was learned as $1$ day, describing nightly dips in temperature. The second term describes longer-term trends in temperature. Its time scale $w\dnt{T}$ was learned as $3$ days, with a relative magnitude $h\dnt{T}$ just over twice that of the periodic term, $h\dnt{P}$. From this we can induce that the periodic term represents only a relatively minor component of this model. Finally, the third term represents higher frequency disturbances, with a time scale learned as $4$ hours. Note that this final term is not correlated amongst sensors as were the other two; these disturbances appear to be local to each sensor. The individual output scales for each sensor given by $W$ were learned as $2\degree$C, $6\degree$C and $2.6\degree$C respectively. These give the scales of the periodic fluctuations and longer-term drifts in the air temperature at each sensor. For comparison, the scale of the disturbances $h\dnt{D}$ were learned as $1\degree$C. As for the tide heights example, only the sensor correlations and delays were sampled over for the depicted predictions.

The predictive performances for this dataset are shown in table \ref{tbl:AT_RMSEs}. The Kalman filter was tested with an order of $p=15$. Note again that the multi-output GP outperforms the other benchmarks with the Kalman filter performing particularly badly as it significantly lags the real temperature measurement.

\begin{figure}
\begin{center}
\begin{tabular}{cc}
\hspace{-1.00cm}\epsfig{figure=figures/cens_GP.eps,width=7.2cm} & \hspace{-0.75cm}\epsfig{figure=figures/cens_KF.eps,width=7.2cm}\\
\hspace{-0.6cm}(a) & \hspace{-0.6cm}(b)
\end{tabular}
\caption{Prediction and regression of air pressure data for (a) a Gaussian process employing censored observations and (b) a Kalman filter.}
\label{censored}
\end{center}
\end{figure}

\subsection{Prediction of Air Pressures}

\noindent Figure \ref{censored} demonstrates regression and prediction over the heavily rounded air pressure observations from the Bramblemet sensor alone. We used a GP model with the Mat\'{e}rn covariance of the non-periodic form shown in equation \eqref{eq:matern}. We further made use of the methods described in Section \ref{sec:censored} for managing the censored (rounded) observations that are all we possess for this data. Here we can demonstrate a dramatic improvement over Kalman filter prediction, as demonstrated in table \ref{tbl:AP_RMSEs}. Both the GP and Kalman filter have an order of $16$; that is, they store only up to the $16$ most recent observations (i.e., $p=16$).

\subsection{Prediction of MIDAS data}

We now turn to a larger sensor network and demonstrate the suitability of our prediction algorithm for such networks. In particular, we apply our algorithm to data from January 1990 collected by the 670 sensors that comprised UK Met Office MIDAS land
surface stations at the time. We arbitrarily selected maximum air temperature as the environmental variable of interest for our experiments, and aimed to perform one-step lookahead prediction. Specifically, we aim to predict the value of the next observation to be received by the network, which may be an observation from a day equal to that of the current observation, but from a sensor different to that most recently observed. The use of a model over both space and time is integral to success on this task.

The dataset comes equipped with latitude and longitude measurements for each sensor, which we can use to express the covariance between any pair of sensors. That is, we take the covariance
\begin{multline} \label{eq:MIDAS_cov}
 K\bigl([t,\,l],\,[t',\,l']\bigr)=
 \Kl{Mtn}\bigl(l,l';h,\,w_l,\nu=\tfrac{3}{2}\bigr)\,
\Kl{Mtn}\bigl(t,t';h=1,\,w_t,\nu=\tfrac{3}{2}\bigr)
\,,
\end{multline}
where for the covariance between $l$ and $l'$ we replace $r$ with the great-circle distance between the positions of sensors $l$ and $l'$, divided by isotropic spatial scale hyperparameter $w_l$. Note that our approach here is an alternative to the arbitrary parameterisation presented in equation \eqref{eq:spherical}. This parameterisation requires a hyperparameter for every distinct pair of sensors and is hence impractical for larger numbers of sensors.

Results from our tests on this dataset are displayed in table \ref{tbl:MIDAS_RMSEs}. Note that the GP is easily able to outperform its naive competitor.

\begin{table}
\centering
\caption{Predictive performances for five-day Bramblemet air temperature dataset.}
\label{tbl:AT_RMSEs}
 \begin{tabular}{@{}lrr@{}}
 \\
 \toprule
Algorithm & RMSE ($\degree\text{C}$) & NMSE (dB)\\
\midrule
Na\"{i}ve & 0.84 & -10.0\\
Kalman filter & 3.94 & 3.1\\
Multi-output GP & 0.45 & -15.4\\
\bottomrule
\end{tabular}
\end{table}

\begin{table}
\centering
\caption{Predictive performances for five-day Bramblemet air pressure dataset.}
\label{tbl:AP_RMSEs}
 \begin{tabular}{@{}lrr@{}}
 \\
 \toprule
Algorithm & RMSE (Pa) & NMSE (dB)\\
\midrule
Na\"{i}ve & 3.61 & -4.75\\
Kalman filter & 3.29 & -5.55\\
GP for censored data & 0.43 & -20.49 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}
\centering
\caption{Predictive performances for MIDAS maximum air temperature data.}
\label{tbl:MIDAS_RMSEs}
 \begin{tabular}{@{}lrr@{}}
\\
\toprule
Algorithm & RMSE ($\degree\text{C}$) & NMSE (dB)\\
\midrule
Na\"{i}ve & 3.93 & 3.61\\
Multi-output GP & 1.53 & -4.57 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Active Data Selection of Tide Heights}

\noindent We now demonstrate our active data selection algorithm. Using the fine-grained data (downloaded directly from the Bramblemet weather sensors), we can simulate how our GP would have chosen its observations had it been in control. Results from the active selection of observations from all the four tide sensors are displayed in figure \ref{active_sampling}. Again, these plots depict dynamic choices; at time $t$, the GP must decide when next to observe, and from which sensor, given knowledge only of the observations recorded prior to $t$, in an attempt to maintain the uncertainty in tide height below 10cm. The covariance function used was that described in \eqref{eq:covtides}.

\begin{figure}
\begin{center}
\begin{tabular}{cc}
\hspace{-0.75cm}\epsfig{figure=figures/indep_tide_1b.eps,width=7.2cm} & \hspace{-1.00cm}\epsfig{figure=figures/dep_tide_1b.eps,width=7.2cm} \\
\hspace{-0.75cm}\epsfig{figure=figures/indep_tide_2b.eps,width=7.2cm} & \hspace{-1.00cm}\epsfig{figure=figures/dep_tide_2b.eps,width=7.2cm} \\
\hspace{-0.75cm}\epsfig{figure=figures/indep_tide_3b.eps,width=7.2cm} & \hspace{-1.00cm}\epsfig{figure=figures/dep_tide_3b.eps,width=7.2cm} \\
\hspace{-0.75cm}\epsfig{figure=figures/indep_tide_4b.eps,width=7.2cm} & \hspace{-1.00cm}\epsfig{figure=figures/dep_tide_4b.eps,width=7.2cm} \\
\hspace{-0.6cm}(a) & \hspace{-0.6cm}(b) \\
\end{tabular}
\caption{Comparison of active sampling of tide data using (a) independent and (b) multi-output Gaussian processes.}
\label{active_sampling}
\end{center}
\end{figure}

Consider first the case shown in figure \ref{active_sampling}(a), in which separate independent GPs are used to represent each sensor. Note that a large number of observations are taken initially as the dynamics of the sensor readings are learnt, followed by a low but constant rate of observation. In contrast, for the multi-output case shown in figure \ref{active_sampling}(b), the GP is allowed to explicitly represent correlations and delays between the sensors. As mentioned above, this data set is notable for the slight delay of the tide heights at the Chimet and Cambermet sensors relative to the Sotonmet and Bramblemet sensors, due to the nature of tidal flows in the area. Note that after an initial learning phase as the dynamics, correlations, and delays are inferred, the GP chooses to sample predominantly from the undelayed Sotonmet and Bramblemet sensors\footnote{The dynamics of the tide height at the Sotonmet sensor are more complex than the other sensors due to the existence of a `young flood stand' and a `double high tide' in Southampton. For this reason, the GP selects Sotonmet as the most informative sensor and samples it most often.}. Despite no observations of the Chimet sensor being made within the time span plotted, the resulting predictions remain remarkably accurate. Consequently only $119$ observations are required to keep the uncertainty below the specified tolerance, whereas $358$ observations were required in the independent case. This represents another clear demonstration of how our prediction is able to benefit from the readings of multiple sensors.


\subsection{Active Data Selection of Wind Speeds}

\begin{figure}
\begin{center}
\begin{tabular}{cc}
\hspace{-0.75cm}\epsfig{figure=figures/indep_ws_1.eps,width=7.2cm} & \hspace{-1.00cm}\epsfig{figure=figures/dep_ws_1.eps,width=7.2cm} \\
\hspace{-0.75cm}\epsfig{figure=figures/indep_ws_2.eps,width=7.2cm} & \hspace{-1.00cm}\epsfig{figure=figures/dep_ws_2.eps,width=7.2cm} \\
\hspace{-0.75cm}\epsfig{figure=figures/indep_ws_3.eps,width=7.2cm} & \hspace{-1.00cm}\epsfig{figure=figures/dep_ws_3.eps,width=7.2cm} \\
\hspace{-0.60cm}(a) & \hspace{-0.60cm}(b) \\
\end{tabular}
\caption{Comparison of active sampling of wind speed using (a) independent and (b) multi-output Gaussian processes.}
\label{active_sampling1}
\end{center}
\end{figure}

Figure \ref{active_sampling1} shows similar results for the wind speed measurements from three of the four sensors (the Cambermet sensor being faulty during this period) where the goal was to maintain the uncertainty in wind speed below $1.5$ knots. By analogy to the covariance function over tides \eqref{eq:covtides}, we used the following covariance function
\begin{multline*} %\label{eq:covWS}
 K\bigl([t,\,l],\,[t',\,l']\bigr)={W}(l,\,l')\,
\Kl{per-Mtn}\bigl(t-\Delta_{l},t'-\Delta_{l'};h\dnt{P}=1,\,w_P,\nu=\tfrac{1}{2}\bigr)+
\\
\dd{l}{l'}\Kl{Mtn}\bigl(t,t';h\dnt{D},\,w\dnt{D},\nu=\tfrac{3}{2}\bigr)
\,.
\end{multline*}

\begin{figure}
\begin{center}
\begin{tabular}{cc}
\hspace{-0.75cm}\epsfig{figure=figures/Wannengrat_station_4.eps,width=7.2cm} & \hspace{-1.00cm}\epsfig{figure=figures/Wannengrat_station_9.eps,width=7.2cm} \\
\hspace{-0.75cm}\epsfig{figure=figures/Wannengrat_station_13.eps,width=7.2cm} & \hspace{-1.00cm}\epsfig{figure=figures/Wannengrat_station_16.eps,width=7.2cm} \\
\hspace{-0.75cm}\epsfig{figure=figures/Wannengrat_station_29.eps,width=7.2cm} & \hspace{-1.00cm}\epsfig{figure=figures/Wannengrat_observations.eps,width=7.2cm} \\
\end{tabular}
\caption{Active sampling of ambient temperatures at $16$ Wannengrat sensor stations.}
\label{active_sampling2}
\end{center}
\end{figure}

In this case, for purposes of clarity, the fine-grained data is not shown on the plot. Note that the measurement noise is much greater in this case, and this is reflected in the uncertainty in the GP predictions. Furthermore, note that while the Sotonmet and Chimet sensors exhibit a noticeable correlation, Bramblemet appears to be relatively uncorrelated with both. This observation is reflected in the sampling that the GP performs. The independent GPs sample the Bramblemet, Sotonmet and Chimet sensors $126$, $120$ and $121$ times respectively, while over the same period, our multi-output GP samples the same sensors $115$, $88$ and $81$ times. Our multi-output GP learns on-line that the wind speed measurements of the Sotonmet and Chimet sensors are correlated, and then exploits this correlation in order to reduce the number of times that these sensors are sampled (inferring the wind speed at one location from observations of another). However, there is little or no correlation between the Bramblemet sensor and the other sensors, and thus, our multi-output GP samples Bramblemet almost as often as the independent GPs.

\subsection{Active Data Selection of Air Temperature}

% The stations available were 3     4     6     8     9    10    11    12    13    14    15    16    18    19    20    29
% Ambient temperature
% 30-Mar-2008
To investigate the practicality of our active data selection algorithm on larger sensor networks, we apply it to data from 16 air temperature sensors from the Wannengrat Alpine Observatory. As in the case of the MIDAS data described above we use the latitude and longitude co-ordinates of these sensors within the covariance function shown in equation \eqref{eq:MIDAS_cov}.

Figure \ref{active_sampling2} shows the detailed predictions and samples for five of the sensors, and a summary plot showing the sample times of all 16 sensors. Note that the sensor sampling rate is relatively low at first, however, as the volatile fluctuations in temperature begin to occur at about $t=0.7$ days, the sampling frequency automatically increases to ensure that the prediction uncertainty is maintained below the specified threshold.

\section{Computation Time}\label{sec_computation}

\noindent As described earlier, a key requirement of our algorithm is computational efficiency, in order that it can be used to represent multiple correlated sensors, and hence, used for real-time information processing. Here we consider the computation times involved in producing the results presented in the previous section. We should note at this point that we are not comparing the computation time of our approach against the Kalman filter. The computation performed by the Kalman filter is far less intensive than that of the Gaussian process approach (in our empirical evaluations, the update time of the Kalman filter was typically $<$ 0.01 seconds). However, as demonstrated in the previous section, the Kalman filter is unable to represent the complex correlations and periodicities seen in the sensor data, and thus, it provides significantly worse predictions than our approach.

Rather, in this section, we are evaluating the computation time required in order to update the Gaussian process as a new observation is received. This computation time represents the cost of computing the Cholesky factor $\mat{R}$ of $\mat{V}$, the associated data-dependent term $\mat{R}(\vxD,\vxD)^{-1}(\vyD-\vect{\mu}(\vxD))$ in equations \eqref{eq:GPMean} and \eqref{eq:GPVar}, and the hyperparameter sample likelihoods and subsequently the weights of equation \eqref{eq:muPsi}\footnote{Once these terms have been calculated, by either method, making predictions at any point in time is extremely fast, requiring simply the adding of another element to $\vxst$.}. The conventional approach to performing this update would be to simply recompute the relevant quantities from scratch upon receipt of new data (using the highly optimised LAPACK routines within Matlab - see \url{http://www.netlib.org/lapack/} for more details of this linear algebra package). In contrast, in our formalism, we make use of the computationally efficient online rules described in Section \ref{sec_efficient} to perform an online update of previously calculated results.  

Thus, against this background, Table \ref{tbl:comp_speed} shows the computation time required using MATLAB on an Intel Core i7 Processor 860 (2.80GHz, 8MB) with 8GB of RAM, with displayed results the average of 10 trials\footnote{Note that these computations would typically be performed at the base station or data portal, and not on the sensors themselves, so reasonable computational resources can be assumed.}. In more detail, Table \ref{tbl:comp_speed}(a) the results when our computationally efficient online rules are used, and Table \ref{tbl:comp_speed}(b) shows the results when the conventional approach is used.

Note that with our efficient updates, we expect the cost of computation to grow as $O(N^2)$ in the number of stored data points. Our proposed algorithm will automatically determine the quantity of data to store in order to achieve the desired level of accuracy. In the problems we have studied, a few hundred points were typically sufficient. The largest number we required was $1500$, for the MIDAS air temperature data. Note also that the cost of computing equation \eqref{eq:KNK} will grow in the cube of the number of samples in each hyperparameter. However, we consider only a fixed set of samples in each hyperparameter, and thus, equation \eqref{eq:KNK} need only be computed once, off-line. In this case, our on-line costs are limited by the multiplication of that term by the likelihoods $\vect{r}_s$ to give the weights of equation \eqref{eq:muPsi}, and this only grows as $O(\eta^2)$. Furthermore, note that this cost is independent of how the $\eta$ samples are distributed amongst the hyperparameters. For completeness, Table \ref{tbl:numbers_of_samples} shows the number of data points and the number of hyperparameter samples used for each of the empirical evaluations.

\begin{table}
\centering
% use packages: array
\caption{The required computation time (seconds) per GP update, with (a) and without (b) our efficient update rules, over $N$ the number of stored data points and $\eta$ the number of hyperparameter samples. } 
\label{tbl:comp_speed} 

\begin{tabular}{ccc}

\begin{tabular}{@{}lrrr@{}}
\\
\toprule
& \multicolumn{3}{c}{$N$}\\
\cmidrule(l){2-4}
$\eta$ & 10 & 100 & 500 \\
\midrule
1 & $<0.01$ & $<0.01$ & $<0.01$\\ 
10 & 0.01 & 0.01 & 0.08 \\
100 & 0.11 & 0.13 & 0.74 \\ 
1000 & 1.30 & 1.39 & 4.51 \\
\bottomrule
\end{tabular}

& \hspace{1cm} &

\begin{tabular}{@{}lrrr@{}}
\\
\toprule
& \multicolumn{3}{c}{$N$}\\
\cmidrule(l){2-4}
$\eta$ & 10 & 100 & 500 \\
\midrule
1 & $<0.01$ & $<0.01$ & 0.06 \\ 
10 & 0.01 & 0.02 & 0.34 \\
100 & 0.09 & 0.17 & 3.32 \\ 
1000 & 0.92 & 1.71 & 17.34 \\
\bottomrule
\end{tabular}

\\
 & & \\
(a) & & (b) \\

\end{tabular}
\end{table}

\begin{table}
\centering
% use packages: array
\caption{The numbers of stored data points, $N$, and hyperparameter samples, $\eta$, used for the various datasets presented in this paper.} 
\label{tbl:numbers_of_samples} 
\begin{tabular}{@{}lrr@{}}
\\
\toprule
Dataset & {$N$} & $\eta$\\
\midrule
Bramblemet Tide Heights & 200 & 1620\\
Bramblemet Air Temperatures & 500 & 972\\
Bramblemet Air Pressures & 16 & 125\\
MIDAS Air Temperature & 1500 & 105 \\
Bramblemet Wind Speeds & 454 & 675\\
Wannengrat Air Temperature & 500 & 125\\
\bottomrule
\end{tabular}
\end{table}

In practice, we see that our efficient updates result in substantial improvements in computational speed once the quantity of data $N$ exceeds the threshold required to offset the small computational overheads introduced by using our approach instead of the already highly optimised LAPACK routines. For the largest cases investigated here we see an approximate four-fold increase in computation speed. For our weather datasets, samples are taken no more frequently than once a minute. Thus all of the information processing that we have described in this paper can easily be performed in real-time, and the increase in computational speed seen in our approach can be used to increase the number of data points considered or the number of hyperparameters samples used; both of which will likely increase the predictive power of the Gaussian process.


\section{Related Work}\label{sec_related}

\noindent Gaussian process regression has a long history of use within geophysics and geospatial statistics (where the process is known as kriging \cite{cressie}), but has only recently been applied within sensor networks. For example, Gaussian processes have been used to represent spatial correlations between sensors so that the near-optimal sensor placement (in terms of mutual information) can be determined, for modelling wireless propagation between sensor nodes subject to censored observations of received signal strength \cite{ertin2007gpm}, and in the form of multi-variate Gaussian distributions to represent correlations between different sensors and sensor types for energy efficient querying of a sensor network \cite{guestrin2}. They have also been used to represent temporal correlations between the readings from a single sensor in order to perform adaptive sampling --- maximising the information gain subject to a sampling constraint \cite{1525857}.

Our work differs in that we use GPs to represent temporal correlations, and represent correlations and delays between sensors with additional hyperparameters. It is thus closely related to other work using GPs to perform regression over multiple responses \cite{dep_GP,latent_factor}. However, our focus is to derive a computationally efficient algorithm, and thus, we use a number of novel computational techniques to allow the re-use of previous calculations as new sensor observations are made. We additionally use a novel Bayesian Monte Carlo technique to marginalise the hyperparameters that describe the correlations and delays between sensors. Finally, we use the variance of the GP's predictions in order to perform active data selection. Likewise, our work differs from that previously using censored sensor readings within a GP framework \cite{ertin2007gpm}, since our work proposes a principled Bayesian Monte Carlo method for adapting our models to the data, where Ertin's model assumes that the hyperparameters are known a priori, and hence, does not consider how likelihoods should be computed in this context. Furthermore, in our work, Monte Carlo techniques are also used to evaluate our other integrals, rather than taking Laplace approximations, allowing more accurate representation of the correlation amongst censored readings. 

Finally, our approach has several advantages relative to sequential state-space models such as the Kalman filter \cite{Girard,Jazwinski} which have also been applied in environmental settings for tracking sensor readings \cite{kalman_oceanography}. Firstly, these state-space models require the discretisation of the time input, representing a discarding of potentially valuable information. Secondly, their sequential nature means they must necessarily perform difficult iterations in order to manage missing or late data, or to produce long-range forecasts. In our GP approach, what observations we have are readily managed, regardless of when they were made. Equally, the computation cost of all our predictions is identical, irrespective of the time or place we wish to make them about. Finally, a sequential framework requires an explicit specification of a transition model. In our approach, we are able to learn a model from data even if our prior knowledge is negligible, and the benefits of our approach are empirically supported by the results presented in Section \ref{sec_evaluation}.

\section{Conclusions}\label{sec_conclusion}

\noindent In this paper we addressed the need for algorithms capable of performing real-time information processing of sensor network data, and we presented a novel computationally efficient formalism of a multi-output Gaussian process. Using weather data collected from three sensor networks, we demonstrated that this formalism allows an end-user to make effective use of sensor data even in the presence of network outages and sensor failures (recovering missing data by making predictions based on previous sensor readings and the current readings of sensors that are functioning), and to automatically determine the sampling rate of each individual sensor in order to ensure that the uncertainty in the environmental parameter being measured stays within a pre-specified limit. Furthermore, we showed that our formalism that efficiently re-uses previous computations by following an online update procedure as new data sequentially arrives results a significant increase in computation speed compared to the highly optimised LAPACK linear algebra package.

Our future work in this area consists of three areas. First, we would like to investigate the use of schemes more sophisticated than the maximum likelihood approach for the movement of the locations of our set of hyperparameter samples. As the posterior distributions of these hyperparameters become more sharply peaked, we might aim to reduce the number of samples to further increase the computational efficiency of our algorithm.

Second, we intend to investigate the use of correlations between different sensor types (rather than between different sensors of the same type as presented here) to perform regression and prediction within our weather sensor network. Our formalism is in no way restricted to identical sensors, and it is expected that many sensors will exhibit correlations in this setting. For example, both wind speed and wind direction, and air temperature and air pressure are likely to exhibit correlations as weather fronts move over the sensor network.

Finally, we would like to use the probabilistic model that the GP builds to automatically handle faulty and unreliable sensors within the network. Note that we do not wish to simply detect these faulty sensors., but would like to simultaneously perform both detection and prediction, despite the presence of these failures. Our preliminary work is this area indicates that nonstationary covariance functions that model such changes can be introduced to the formalism described here to achieve this, and have already been shown to work effectively on real-world sensor data derived from the sensor networks described in this paper \cite{changepoint}.

\section*{Acknowledgments}

\noindent This research was undertaken as part of the ALADDIN (Autonomous Learning Agents for Decentralised Data and Information Networks) project and is jointly funded by a BAE Systems and EPSRC strategic partnership (EP/C548051/1). We would like to thank B. Blaydes of the Bramblemet/Chimet Support Group, and W. Heaps of Associated British Ports (ABP) for allowing us access to the weather sensor network, hosting our RDF data on the sensor web sites, and for providing raw sensor data as required. We also thank the UK Met office for the use of
the MIDAS dataset.

 
\bibliographystyle{acmtrans}
\bibliography{tosn_gp}

% \begin{thebibliography}{}
% 
% \bibitem[\protect\citeauthoryear{Abrahamsen}{Abrahamsen}{1997}]{Abrahamsen}
% {\sc Abrahamsen, P.} 1997.
% \newblock A review of {G}aussian random fields and correlation functions.
% \newblock Tech. Rep. 917, Norwegian Computing Center, Box 114, Blindern, N-0314
%   Oslo, Norway.
% \newblock 2nd edition.
% 
% \bibitem[\protect\citeauthoryear{Barnaghi, Meissner, Presser, and
%   Moessner}{Barnaghi et~al\mbox{.}}{2009}]{sensor_rdf}
% {\sc Barnaghi, P.}, {\sc Meissner, S.}, {\sc Presser, P.}, {\sc and} {\sc
%   Moessner, K.} 2009.
% \newblock {Sense and sens'ability: Semantic data modelling for sensor
%   networks}.
% \newblock In {\em Proceedings of the ICT Mobile and Wireless Communications
%   Summit}. Santander, Spain.
% 
% \bibitem[\protect\citeauthoryear{Bertino, Evensen, and Vackernagel}{Bertino
%   et~al\mbox{.}}{2003}]{kalman_oceanography}
% {\sc Bertino, L.}, {\sc Evensen, G.}, {\sc and} {\sc Vackernagel, H.} 2003.
% \newblock Sequential data assimilation techniques in oceanography.
% \newblock {\em International Statistical Review\/}~71, 223--242.
% 
% \bibitem[\protect\citeauthoryear{Boyle and Frean}{Boyle and
%   Frean}{2005}]{dep_GP}
% {\sc Boyle, P.} {\sc and} {\sc Frean, M.} 2005.
% \newblock Dependent {G}aussian processes.
% \newblock In {\em Advances in Neural Information Processing Systems 17}. The
%   MIT Press, 217--224.
% 
% \bibitem[\protect\citeauthoryear{Cressie}{Cressie}{1991}]{cressie}
% {\sc Cressie, N. A.~C.} 1991.
% \newblock {\em Statistics for spatial data}.
% \newblock John Wiley \& Sons.
% 
% \bibitem[\protect\citeauthoryear{Deshpande, Guestrin, Madden, Hellerstein, and
%   Hong}{Deshpande et~al\mbox{.}}{2004}]{guestrin2}
% {\sc Deshpande, A.}, {\sc Guestrin, C.}, {\sc Madden, S.}, {\sc Hellerstein,
%   J.}, {\sc and} {\sc Hong, W.} 2004.
% \newblock Model-driven data acquisition in sensor networks.
% \newblock In {\em Proceedings of the Thirtieth International Conference on Very
%   Large Databases}. Toronto, Canada, 588--599.
% 
% \bibitem[\protect\citeauthoryear{Ertin}{Ertin}{2007}]{ertin2007gpm}
% {\sc Ertin, E.} 2007.
% \newblock {Gaussian process models for censored sensor readings}.
% \newblock In {\em Proceedings of the Fourteenth IEEE/SP Workshop on Statistical
%   Signal Processing}. Madison, Wisconsin, USA, 665--669.
% 
% \bibitem[\protect\citeauthoryear{Fuentes, Chaudhuri, and Holland}{Fuentes
%   et~al\mbox{.}}{2007}]{fuentes}
% {\sc Fuentes, M.}, {\sc Chaudhuri, A.}, {\sc and} {\sc Holland, D.~H.} 2007.
% \newblock Bayesian entropy for spatial sampling design of environmental data.
% \newblock {\em Environmental and Ecological Statistics\/}~14, 323--340.
% 
% \bibitem[\protect\citeauthoryear{Garnett, Osborne, and Roberts}{Garnett
%   et~al\mbox{.}}{2009}]{changepoint}
% {\sc Garnett, R.}, {\sc Osborne, M.~A.}, {\sc and} {\sc Roberts, S.~J.} 2009.
% \newblock Sequential Bayesian prediction in the presence of changepoints.
% \newblock In {\em Proceedings of the Twenty-Sixth International Conference on Machine
%   Learning}. Montreal, Canada.
% 
% \bibitem[\protect\citeauthoryear{Genz}{Genz}{1992}]{genz1992ncm}
% {\sc Genz, A.} 1992.
% \newblock {Numerical computation of multivariate normal probabilities}.
% \newblock {\em Journal of Computational and Graphical Statistics\/}~{\em
%   1,\/}~2, 141--149.
% 
% \bibitem[\protect\citeauthoryear{Girard, Rasmussen, Candela, and
%   Murray-Smith}{Girard et~al\mbox{.}}{2003}]{Girard}
% {\sc Girard, A.}, {\sc Rasmussen, C.}, {\sc Candela, J.}, {\sc and} {\sc
%   Murray-Smith, R.} 2003.
% \newblock {G}aussian process priors with uncertain inputs -- application to
%   multiple-step ahead time series forecasting.
% \newblock In {\em Advances in Neural Information Processing Systems 16}. MIT
%   Press, Cambridge, MA, 545--552.
% 
% \bibitem[\protect\citeauthoryear{Hart and Martinez}{Hart and
%   Martinez}{2006}]{esn}
% {\sc Hart, J.~K.} {\sc and} {\sc Martinez, K.} 2006.
% \newblock {Environmental sensor networks: A revolution in the earth system
%   science?}
% \newblock {\em {Earth-Science Reviews}\/}~{\em 78}, 177--191.
% 
% \bibitem[\protect\citeauthoryear{Jazwinski}{Jazwinski}{1970}]{Jazwinski}
% {\sc Jazwinski, A.} 1970.
% \newblock {\em Stochastic processes and filtering theory}.
% \newblock Academic Press New York.
% 
% \bibitem[\protect\citeauthoryear{Kho, Rogers, and Jennings}{Kho
%   et~al\mbox{.}}{2009}]{1525857}
% {\sc Kho, J.}, {\sc Rogers, A.}, {\sc and} {\sc Jennings, N.~R.} 2009.
% \newblock Decentralized control of adaptive sampling in wireless sensor
%   networks.
% \newblock {\em ACM Transactions on Sensor Networks\/}~{\em 5,\/}~3, 1--35.
% 
% \bibitem[\protect\citeauthoryear{Krause, Guestrin, Gupta, and Kleinberg}{Krause
%   et~al\mbox{.}}{2006}]{guestrin1}
% {\sc Krause, A.}, {\sc Guestrin, C.}, {\sc Gupta, A.}, {\sc and} {\sc
%   Kleinberg, J.} 2006.
% \newblock Near-optimal sensor placements: Maximizing information while
%   minimizing communication cost.
% \newblock In {\em Proceedings of the Fifth International Conference on
%   Information Processing in Sensor Networks}. Nashville, Tennessee, USA, 2--10.
% 
% \bibitem[\protect\citeauthoryear{Lassila and Swick}{Lassila and
%   Swick}{1999}]{rdf}
% {\sc Lassila, O.} {\sc and} {\sc Swick, R.~R.} 1999.
% \newblock Resource description framework (RDF) model and syntax specification.
% \newblock Available at
%  \url{http://www.w3.org/TR/1999/REC-rdf-syntax-19990222/}.
% 
% \bibitem[\protect\citeauthoryear{Lee and Roberts}{Lee and
%   Roberts}{2008}]{MinTechReport}
% {\sc Lee, S.~M.} {\sc and} {\sc Roberts, S.~J.} 2008.
% \newblock Multivariate time series forecasting in incomplete environments.
% \newblock Tech. Rep. PARG-08-03. Available at
%   \url{www.robots.ox.ac.uk/~parg/publications.html},
%   University of Oxford.
% 
% \bibitem[\protect\citeauthoryear{MacKay}{MacKay}{2002}]{MKBook}
% {\sc MacKay, D. J.~C.} 2002.
% \newblock {\em Information Theory, Inference \& Learning Algorithms}.
% \newblock {Cambridge University Press}.
% 
% \bibitem[\protect\citeauthoryear{O'Hagan}{O'Hagan}{1987}]{MCUnsound}
% {\sc O'Hagan, A.} 1987.
% \newblock {Monte Carlo is fundamentally unsound}.
% \newblock {\em The Statistician\/}~{\em 36}, 247--249.
% 
% \bibitem[\protect\citeauthoryear{Padhy, Dash, Martinez, and Jennings}{Padhy
%   et~al\mbox{.}}{2010}]{usac}
% {\sc Padhy, P.}, {\sc Dash, R.~K.}, {\sc Martinez, K.}, {\sc and} {\sc
%   Jennings, N.~R.} 2010.
% \newblock A utility-based adaptive sensing and multi-hop communication protocol
%   for wireless sensor networks.
% \newblock {\em ACM Transactions on Sensor Networks\/}~{\em 6,\/}~3.
% \newblock In print.
% 
% \bibitem[\protect\citeauthoryear{Pinheiro and Bates}{Pinheiro and
%   Bates}{1996}]{PinheiroBates}
% {\sc Pinheiro, J.} {\sc and} {\sc Bates, D.} 1996.
% \newblock Unconstrained parameterizations for variance-covariance matrices.
% \newblock {\em Statistics and Computing\/}~{\em 6}, 289--296.
% 
% \bibitem[\protect\citeauthoryear{Rasmussen and Ghahramani}{Rasmussen and
%   Ghahramani}{2003}]{BZMonteCarlo}
% {\sc Rasmussen, C.~E.} {\sc and} {\sc Ghahramani, Z.} 2003.
% \newblock {{B}ayesian Monte Carlo}.
% \newblock In {\em Advances in Neural Information Processing Systems 15}. The
%   MIT Press, 489--496.
% 
% \bibitem[\protect\citeauthoryear{Rasmussen and Williams}{Rasmussen and
%   Williams}{2006}]{GPsBook}
% {\sc Rasmussen, C.~E.} {\sc and} {\sc Williams, C. K.~I.} 2006.
% \newblock {\em {G}aussian Processes for Machine Learning}.
% \newblock MIT Press.
% 
% \bibitem[\protect\citeauthoryear{Sasena}{Sasena}{2002}]{Sasena}
% {\sc Sasena, M.~J.} 2002.
% \newblock {Flexibility and Efficiency Enhancements for Constrained Global
%   Design Optimization with Kriging Approximations}.
% \newblock Ph.D. thesis, University of Michigan.
% 
% \bibitem[\protect\citeauthoryear{Stein}{Stein}{2005}]{stein2005stc}
% {\sc Stein, M.} 2005.
% \newblock {Space-Time Covariance Functions.}
% \newblock {\em Journal of the American Statistical Association\/}~{\em
%   100,\/}~469, 310--322.
% 
% \bibitem[\protect\citeauthoryear{Teh, Seeger, and Jordan}{Teh
%   et~al\mbox{.}}{2005}]{latent_factor}
% {\sc Teh, Y.~W.}, {\sc Seeger, M.}, {\sc and} {\sc Jordan, M.~I.} 2005.
% \newblock Semiparametric latent factor models.
% \newblock In {\em Proceedings of the Conference on Artificial Intelligence and
%   Statistics}. Barbados, 333--340.
% 
% \bibitem[\protect\citeauthoryear{{The MathWorks}}{{The
%   MathWorks}}{2007}]{Matlab}
% {\sc {The MathWorks}}. 2007.
% \newblock {MATLAB R2007a}.
% \newblock Natick, MA.
% 
% \end{thebibliography}


\appendix

\section{Appendix} \label{sec:Appendix}

\subsection{Cholesky Factor Update} \label{sec:CholeskyUpdate}

\small

\noindent We have a positive definite matrix, represented in block form as $\begin{bmatrix} V_{1,1} & V_{1,3} \\ V^\tr_{1,3} & V_{3,3} \end{bmatrix}$ and its Cholesky factor, $\begin{bmatrix} R_{1,1} & R_{1,3} \\ 0 & R_{3,3} \end{bmatrix}$. Given a new positive definite matrix, which differs from the old only in the insertion of some new rows and columns,$\begin{bmatrix} V_{1,1} & V_{1,2} & V_{1,3} \\ V^\tr_{1,2} & V_{2,2} & V_{2,3} \\ V^\tr_{1,3} & V^\tr_{2,3} & V_{3,3} \end{bmatrix}
$, we wish to efficiently determine its Cholesky factor, $\begin{bmatrix} S_{1,1} & S_{1,2} & S_{1,3} \\ 0 & S_{2,2} & S_{2,3} \\ 0 & 0 & S_{3,3} \end{bmatrix}
$. For $\mat{A}$ triangular, we define $\vx=\mat{A}\setminus \vect{b}$ as the solution to the equations $\mat{A}\,\vx=\vect{b}$ as found by the use of backwards or forwards substitution. The following rules are readily obtained
\begin{align}
 S_{1,1}&=R_{1,1}\\
S_{1,2} &=R^\tr_{1,1}\setminus V_{1,2}\\
S_{1,3} &=R_{1,3}\\
S_{2,2} &=\chol (V_{2,2}-S^\tr_{1,2}S_{1,2})\\
S_{2,3} &=S^\tr_{2,2}\setminus (V_{2,3}-S^\tr_{1,2}S_{1,3})\\
S_{3,3} &=\chol (R^\tr_{3,3}R_{3,3}-S^\tr_{2,3}S_{2,3}) \,.
\end{align}
By setting the appropriate row and column dimensions (to zero if necessary), this allows us to efficiently determine the Cholesky factor given the insertion of rows and columns in any position. 

\subsection{Data Term Update} \label{sec:DataTermUpdate}

\noindent We have all terms defined in Section \ref{sec:CholeskyUpdate}, in addition to $\begin{bmatrix} Y_1\\Y_2\\Y_3 \end{bmatrix}$ and the product $\begin{bmatrix} C_1\\C_3 \end{bmatrix}\defequal\begin{bmatrix} R_{1,1} & R_{1,3} \\ 0 & R_{3,3} \end{bmatrix}^{-1} \begin{bmatrix} Y_1\\Y_3 \end{bmatrix}$. To efficiently determine $\begin{bmatrix} D_1\\D_2\\D_3 \end{bmatrix}\defequal\begin{bmatrix} S_{1,1} & S_{1,2} & S_{1,3} \\ 0 & S_{2,2} & S_{2,3} \\ 0 & 0 & S_{3,3} \end{bmatrix}^{-1} \begin{bmatrix} Y_1\\Y_2\\Y_3 \end{bmatrix}$, we have

\begin{align}
 D_{1}&=C_{1}\\
D_{2} &=S_{2,2}^{-\tr}\big(Y_2-S_{1,2}^\tr\,C_1\big)\\
D_{3}&=S_{3,3}^{-\tr}\big(R_{3,3}^\tr \, C_3-S_{2,3}^\tr\,D_2\big)\,.
\end{align}

\subsection{Log-Gaussian Update} \label{sec:LogGaussianUpdate}

We have all terms defined in Sections \ref{sec:CholeskyUpdate} and \ref{sec:DataTermUpdate}, in addition to 
\begin{align*}
K & = \log \N{\begin{bmatrix} Y_1\\Y_3 \end{bmatrix}}
{\begin{bmatrix}0\\0\end{bmatrix}}
{\begin{bmatrix} V_{1,1} & V_{1,3} \\ V^\tr_{1,3} & V_{3,3} \end{bmatrix}}\\
& = -\frac{1}{2} \trace \log (\sqrt{2 \pi} R_{1,1}) -\frac{1}{2} \trace \log (\sqrt{2 \pi} R_{3,3}) - \frac{1}{2} C_1^\tr\, C_1 - \frac{1}{2} C_3^\tr\, C_3\,.
\end{align*}
We can then calculate the updated term
\begin{align}
L & = \log \N{\begin{bmatrix} Y_1\\Y_2\\Y_3 \end{bmatrix}}
{\begin{bmatrix}0\\0\\0\end{bmatrix}}
{\begin{bmatrix} V_{1,1} & V_{1,2} & V_{1,3} \\ V^\tr_{1,2} & V_{2,2} & V_{2,3} \\ V^\tr_{1,3} & V^\tr_{2,3} & V_{3,3} \end{bmatrix}}
\nonumber\\
& = -\frac{1}{2} \trace \log (\sqrt{2 \pi} S_{1,1}) -\frac{1}{2} \trace \log (\sqrt{2 \pi} S_{2,2})-\frac{1}{2} \trace \log (\sqrt{2 \pi} S_{3,3}) 
\nonumber\\
&\phantom{=}\; - \frac{1}{2} D_1^\tr\, D_1 - \frac{1}{2} D_2^\tr\, D_2- \frac{1}{2} D_3^\tr\, D_3
\nonumber\\
& = K + \frac{1}{2} \trace \log (\sqrt{2 \pi} R_{3,3}) - \frac{1}{2} \trace \log (\sqrt{2 \pi} S_{2,2})-\frac{1}{2} \trace \log (\sqrt{2 \pi} S_{3,3})
\nonumber\\
&\phantom{=}\; + \frac{1}{2} C_3^\tr\, C_3 - \frac{1}{2} D_2^\tr\, D_2- \frac{1}{2} D_3^\tr\, D_3
\end{align}

\subsection{Cholesky Factor Downdate} \label{sec:CholeskyDowndate}

\noindent We have a positive definite matrix, represented in block form as $\begin{bmatrix} V_{1,1} & V_{1,2} & V_{1,3} \\ V^\tr_{1,2} & V_{2,2} & V_{2,3} \\ V^\tr_{1,3} & V^\tr_{2,3} & V_{3,3} \end{bmatrix}
$ and its Cholesky factor, $
\begin{bmatrix} S_{1,1} & S_{1,2} & S_{1,3} \\ 0 & S_{2,2} & S_{2,3} \\ 0 & 0 & S_{3,3} \end{bmatrix}
$. Given a new positive definite matrix, which differs from the old only in the deletion of some new rows and columns, $\begin{bmatrix} V_{1,1} & V_{1,3} \\ V^\tr_{1,3} & V_{3,3} \end{bmatrix}$, we wish to efficiently determine its Cholesky factor $\begin{bmatrix} R_{1,1} & R_{1,3} \\ 0 & R_{3,3} \end{bmatrix}$. The following rules are readily obtained
\begin{align}
 R_{1,1}&=S_{1,1}\\
R_{1,3} &=S_{1,3}\\
R_{3,3}&=\chol(S^\tr_{2,3}S_{2,3}+S^\tr_{3,3}S_{3,3}) \label{eq:R33}\,.
\end{align}
Note that the special structure of equation \eqref{eq:R33} can be exploited for the efficient resolution of the required Cholesky operation, as, for example, in the MATLAB  function $\operatorname{cholupdate}$  \cite{Matlab}. By setting the appropriate row and column dimensions (to zero if necessary), this allows us to efficiently determine the Cholesky factor given the deletion of rows and columns in any position. 



\end{document}